<!DOCTYPE html>
<html lang="en">
<head>
  <link rel="stylesheet" href="/static/water.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">
  <title>cka prep</title>
  <link rel="stylesheet" href="/static/highlight.min.css">
  <script src="/static/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>

    body {
      font-family: open sans, "SF Pro Display", system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue";
      max-width: 960px;
      margin: 0 auto;
    }
    h1 > a, h2 > a, h3 > a, h4 > a, h5 > a, h6 > a {
      color: black;
      text-decoration: none;
    }
    h1 > a:hover, h2 > a:hover, h3 > a:hover, h4 > a:hover, h5 > a:hover, h6 > a:hover {
      text-decoration: underline;
    }
    h1 > a:visited, h2 > a:visited, h3 > a:visited, h4 > a:visited, h5 > a:visited, h6 > a:visited {
      color: black;
    }

    a:visited {
      color: blue;
    }

    .important {
      background-color: lightgray;
      padding: 1em
    }

    .chapter {
      font-style: italic;

    }
  </style>
</head>
<body>
<a href='/'>
  < back home
</a>
<hr>
<small><i>
Crated on <time datetime="2025-07-18">July 18, 2025</time>; Last modified on
<time datetime="2025-08-09">Aug 9, 2025.</time>
</i></small>
<br>
<small>
This used to be a different page. Go to <a href="/posts/cka-old.html">cka old
notes</a> to check those out.
</small>

<h1 id='top'>~ cka prep</h1>
<a href='#table'>Table of Contents</a>

<h2 id='exam_details'><a href='#exam_details'>Sections</a></h2>
<section>
<ul>
  <li> 25% Cluster Arch, Installation and Configuration </li>
  <li> 15% Workloads and Scheduling </li>
  <li> 20% Services and Networking </li>
  <li> 10% Storage </li>
  <li> 30% Troubleshooting </li>
</ul>
</section>

<h2 id='k8s_nutshell'><a href='#k8s_nutshell'>K8s in a Nutshell</a></h2>
<section>
<p>
K8s is a container orchestration tool. Today there are a lot of microservices
architectures. We can have a container per service and managing all those
containers can be hard. K8s takes care of the scalability, security,
persistence and load balancing.
</p>
<p>
When k8s is triggered to create a container, it will delegate it to the
container runtime engine via a CRI (container runtime interface).
</p>

<h3 id='features'><a href='#features'>Features</a></h3>
<ul>
  <li>
    <b>Declarative Model</b>
    <ul>
      <li>
        the cool thing about k8s, is that you just tell it the status
        of the cluster you want, via yamls, and it will do its best to
        create it.
      </li>
    </ul>
  </li>
  <li>
    <b>Autoscaling</b>
    <ul>
      <li>
        k8s can also automatically (or manually) scale resources when
        needed.
      </li>
    </ul>
  </li>
  <li>
    <b>Application management</b>
    <ul>
      <li>
        When deploying new versions for your application, you can use
        k8s to manage the roll out technique. As well as come back to
        previous rollouts
      </li>
    </ul>
  </li>
  <li>
    <b>Persistent Storage</b>
    <ul>
      <li>
        Containers are ephemeral, meaning the filesystem in them will
        die with them. With k8s you can have persistent volumes, to
        manage storage across containers.
      </li>
    </ul>
  </li>
  <li>
    <b>Networking</b>
    <ul>
      <li>
        k8s has internal and external load balancing for network
        traffic.
      </li>
    </ul>
  </li>
</ul>

<h3 id='high_level_arch'><a href='#high_level_arch'>High-Level Architecture</a></h3>
<p>
There are two types of nodes (these can be vm, baremetal machines, whatever
you call a computer):
</p>
<ul>
  <li>
    <b>Control plane nodes</b>
    <ul>
      <li>
        This node exposes k8s API thru a server. Whenever you do
        <code>kubectl something</code> you are talking to this API.
      </li>
    </ul>
  </li>
  <li>
    <b>Worker nodes</b>
    <ul>
      <li>
        These are the nodes that execute the workload in containers
        managed by pods. Worth noting that every worker node needs a
        container runtime engine, to create the containers.
      </li>
    </ul>
  </li>
</ul>

<h3 id='control_plane_nodes'><a href='#control_plane_nodes'>Control Plane Nodes</a></h3>
<p>These have different components to do their job:</p>
<ul>
  <li>
    <b>API Server</b>
    <ul>
      <li>
        Exposes the k8s api to clients (kubectl)
      </li>
      <li>
        Here is where the authentication, authorization and admission
        control also happens.
      </li>
    </ul>
  </li>
  <li>
    </b>Scheduler</b>
    <ul>
      <li>
        Background process that watches for new k8s pods with no
        assigned and look for a node to execute them.
      </li>
    </ul>
  </li>
  <li>
    <b>Control Manager</b>
    <ul>
      <li>
        Watches the state of your clusters and implements changes where
        needed.
      </li>
      <li>
        This is the guy that makes the cluster be to its desired state.
      </li>
    </ul>
  </li>
  <li>
    <b>Etcd</b>
    <ul>
      <li>
        key-value db that stores all k8s cluster related data.
      </li>
    </ul>
  </li>
</ul>

<h3 id='shared_components'><a href='#shared_components'>Components Shared by Nodes</a></h3>
<p>
There are some components that are shared by the nodes, whether they are
control or workers.
</p>
<ul>
  <li>
    <b>Kubelet</b>
    <ul>
      <li>
        agent that makes sure the necessary containers are running in a
        pod.
      </li>
      <li>
        this is the glue between k8s and the container runtime engine.
      </li>
    </ul>
  </li>
  <li>
    <b>Kube Proxy</b>
    <ul>
      <li>
        every node has this network proxy to enable network
        communication and implement their rules.
      </li>
    </ul>
  </li>
  <li>
    <b>Container runtime</b>
    <ul>
      <li>
        the container runtime that will manage the containers. Not
        really needed in control planes.
      </li>
    </ul>
  </li>
</ul>

<h3 id='advantages'><a href='#advantages'>Advantages on Using k8s</a></h3>
<ul>
  <li>
    <b>Portability</b>
    <ul>
      <li>
        the container runtime engine can run stuff regardless of whether
        you run it on a vm, bm or even on a raspberry. So you have
        portability for your things regardless of where the cluster is
        located.
      </li>
    </ul>
  </li>
  <li>
    <b>Resilience</b>
    <ul>
      <li>
        controllers are always looking to be on the desired state.
        Meaning it will try to self-heal when something goes array.
      </li>
    </ul>
  </li>
  <li>
    <b>Scalability</b>
    <ul>
      <li>
        k8s can scale the number of pods on demand or automatically
      </li>
    </ul>
  </li>
  <li>
    <b>Extensibility</b>
    <ul>
      <li>
        When the core functionality is not enough you can introduce CRDs
        that extend the functionality of the cluster.
      </li>
    </ul>
  </li>
</ul>
</section>

<h2 id='interacting_k8s'><a href='#interacting_k8s'>Interacting with K8s</a></h2>
<section>

<h3 id='api_primitives'><a href='#api_primitives'>API Primitives and Objects</a></h3>
<p>
K8s has api resources which are the building blocks of the cluster. These are
your pods, deployments, services, and so on.
</p>
<p>Every k8s primitive follows a general structure:</p>
<ul>
  <li>
    <i>api version:</i>
    <ul>
      <li>
        defines the structure of a primitive and uses it to validate the
        correctness of the data.
      </li>
      <li>
        you can do <code>k api-versions</code> to see the versions
        compatible with your cluster
      </li>
    </ul>
  </li>
  <li>
    <i>kind</i>
    <ul>
      <li>
        the type of the primitive
      </li>
    </ul>
  </li>
  <li>
    <i>metadata</i>
    <ul>
      <li>
        name, namespace, high level info
      </li>
      <li>
        here you see the UID, which is an id k8s generates for each
        object.
      </li>
    </ul>
  </li>
  <li>
    <i>spec</i>
    <ul>
      <li>
        the desired state of the resource
      </li>
    </ul>
  </li>
  <li>
    <i>status</i>
    <ul>
      <li>
        actual state of the resource
      </li>
    </ul>
  </li>
</ul>

<h3 id='kubectl'><a href='#kubectl'>kubectl</a></h3>
<p>
This is how we talk to the api. Usually you do:
</p>
<pre><code>k &lt;verb&gt; &lt;resource&gt; &lt;name&gt;</code></pre>
<p>
Keep in mind we usually have different stuff in different namespaces, so we
are always appending <code>-n &lt;some namespace&gt;</code> to the command.
</p>
<p>
The name of an object has to be unique across all objects of the same resource
within a namespace.
</p>

<h3 id='managing_objects'><a href='#managing_objects'>Managing Objects</a></h3>
<p>
There are two ways to manage objects: the imperative or the declarative way.
</p>

<h4 id='imperative'><a href='#imperative'>Imperative</a></h4>
<p>
The imperative is where you use commands to make stuff happen in the cluster.
Say you want to create an nginx pod you would do:
</p>
<pre><code>k run --image=nginx:latest nginx --port=80</code></pre>
<p>
This would create the pod in the cluster when you hit enter. In my
professional experience, you hardly ever create stuff like that. The only
time I use it is to create temporary pods to test something.
</p>
<p>
There are other verbs which you might use a bit more. <code>edit</code> brings
up the raw config of the resource and you can change it on the fly. Although
I would recommend just do this for testing things. Hopefully your team has
the manifests under a version control system, if you edit stuff like this it
would mess it up.
</p>
<p>
There is also <code>patch</code> which I have never used, but it... "Update
fields of a resource using strategic merge patch, a JSON merge patch, or a
JSON patch."
</p>
<p>
There is also <code>delete</code> which -- as you probably guess already --
deletes the resource. Usually the object gets a 30 sec grace period for it to
die. But if it does not the kubelet will try to kill it forcefully.
</p>
<p>If you do:</p>
<pre><code>k delete pod nginx --now</code></pre>
<p>It will ignore the grace period.</p>

<h4 id='declarative'><a href='#declarative'>Declarative</a></h4>
<p>
This is where you have a bunch of <code>yaml</code>s which are your
definitions of resources. The cool thing about this is that you can version
control them. Say you have a <code>nginx-deploy.yaml</code>. You can create
it in the cluster with:
</p>
<pre><code>k apply -f nginx-deploy.yaml</code></pre>
<p>
This gives you more flexibility on what you are doing. Since you can just go
to the file change stuff and apply it again.
</p>

<h4 id='hybrid'><a href='#hybrid'>Hybrid</a></h4>
<p>
Usually I use a hybrid approach, most of the imperative commands have this
<code>--dry-run=client -o yaml</code> flag that you can append to the command
and it will render the yaml manifest. You can redirect that to a file and
start working on that. You open the yaml with your favourite text editor, and
then mount volumes and stuff like that.
</p>
<p>
There are more ways to manage the resources for example you can use kustomize
to render different values based on the same manifest, or with helm to bring
up complete apps/releases to just cluster. Probably we will go over them
later in the book.
</p>
</section>

<h2 id='cluster_installation'><a href='#cluster_installation'>Cluster Installation and Upgrade</a></h2>
<section>

<h3 id='provision_infra'><a href='#provision_infra'>Get Infrastructure Ready</a></h3>

<p>
There are a million ways of doing this. I used terraform to create some
droplets in digital ocean and packer with ansible to build an image that would
let everything ready for me to run the <code>kubeadm</code> commands.
</p>

<p>
<code>kubeadm</code> is the tool to create a cluster.
</p>

<p>
Here is a non-comprehensive list of what is needed before running
<code>kubeadm</code> stuff.

<ul>
    <li>
        <p>
            Open <a
            href='https://kubernetes.io/docs/reference/networking/ports-and-protocols/'>ports
            needed</a> for k8s to work
        </p>
    </li>
    <li>
        <p>Disable swap; otherwise kubelet is going to fail to start</p>
    </li>
    <li>
        <p>Install a container runtime, like containerd</p>
    </li>
    <li>
        <p>Install <code>kubeadm</code></p>
    </li>
</ul>
</p>


<h3 id='extension_interfaces'><a href='#extension_interfaces'>Extension Interfaces</a></h3>
<p>
There are some things k8s does not have by default. You need to install this
extensions as needed.
<ul>
  <li>
    <p>
      Container Network Interface (CNI)
      <ul>
        <li>
            This manages the network interaction between containers.
        </li>
      </ul>
    </p>
  </li>
  <li>
    <p>
      Container Network Interface (CRI)
      <ul>
        <li>
            This is the piece that interacts with the Container Runtime
            (containerd, or other) to tell it what to do. Kill, create
            containers and so on.
        </li>
      </ul>
    </p>
  </li>
  <li>
    <p>
      Container Storage Interface (CSI)
      <ul>
        <li>
          This is the standard to implement plugins for interacting with
          block/file storage
        </li>
      </ul>
    </p>
  </li>
</ul>
</p>

<h3 id='setup_cluster'><a href='#setup_cluster'>Setup Cluster</a></h3>

<p>
Once you have <code>kubeadm</code> in your system everything else is pretty
straight forward. You just ssh to your control plane and run:
<pre>
<code>
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
</code>
</pre>
</p>

<p>
This runs some preflight checks to see if everything is working properly, if
not it will likely print a message telling you about what is wrong. In my case
it complained about <code>/proc/sys/net/ipv4/ip_forward</code> being disabled.
But was able to fix it by just doing <code>echo 1 | sudo tee
/proc/sys/net/ipv4/ip_forward</code>.
</p>

<p>
Where does the <code>cidr</code> comes from? I had exactly the same question.
It seems that it will depend on the CNF you will install, but do not quote me
on that.
</p>

<p>
Once the command runs successfully, it will print next steps:
<pre>
<code>
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join ip.control.panel.and:port --token some-cool-token \
	--discovery-token-ca-cert-hash sha256:some-cool-hash
</code>
</pre>
</p>

<p>
Just follow those steps. You ssh into the workers and join them with that
command. If you lost the tokens for some reason you can reprint them with:
<pre>
<code>
kubeadm token create --print-join-command
</code>
</pre>
</p>

<p>
Now, before joining the workers, you need to install the CNI, you can pick any
of the ones on <a
href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">the
k8s add-ons docs</a>.
</p>

<p>
Installing them is nothing fancy, your literally just run a `k apply -f
some-mainfest` and be done with it. I went with <a
href="https://docs.tigera.io/calico/latest/getting-started/kubernetes/flannel/install-for-flannel">calico</a>
for no particular reason.
</p>

<h3 id='highly_available'><a href='#highly_available'>Highly Available (HA) Cluster</a></h3>

<p>
The control plane is like the most important part of the cluster, since if it
fails, you are not even going to be able to talk to the API to do stuff. We can
add redundancy to improve this. Here is where HA architectures come into play.
</p>

<p>
There are two
<ul>
    <li>
        <p>Stacked etcd topology</p>
    </li>
    <li>
        <p>External etcd topology</p>
    </li>
</ul>
Remember <i>etcd</i> is key-value db k8s uses to store all of its info.
</p>

<h4 id='stacked_etcd'><a href='#stacked_etcd'>Stacked etcd topology</a></h4>
<p>
You have at least three control planes each with its own etcd in the same node.
All the nodes running at the same time and the workers talk to them through a
load balancer, if one dies, we still have others.
</p>
<img src="/static/img/cka/kubeadm-ha-topology-stacked-etcd.svg" alt="stacked etcd topology">
<small>
Diagram representing this arch, that I stole from the <a href="https://kubernetes.io/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg">k8s docs</a>
on this topic.
</small>
<hr>

<h4 id='external_etcd'><a href='#external_etcd'>External etcd topology</a></h4>
<p>
Per control plane we have two nodes, one that runs etcd and one that runs the
actual control plane stuff. They communicate through the
<code>kube-apiserver</code> api.
</p>
<p>
This topology require more nodes, and that means a bit more manage overhead.
</p>
<img src="/static/img/cka/kubeadm-ha-topology-external-etcd.svg" alt="stacked etcd topology">
<small>
Diagram representing this arch, that I stole from the <a href="https://kubernetes.io/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg">k8s docs</a>
on this topic.
</small>
<hr>

<h3 id='upgrading_cluster'><a href='#upgrading_cluster'>Upgrading Cluster Version</a></h3>
<p>
It is recommended to upgrade from a minor version to a next higher one, say,
<code>1.18.0</code> to <code>1.19.0</code>,  or from a patch version to a
higher one, <code>1.18.0</code> to <code>1.18.3</code>
</p>

<p>
The high level plan is this:
<ol>
    <li>
        <p>Upgrade a primary control plane node</p>
    </li>
    <li>
        <p>In case of HA, upgrade additional control planes</p>
    </li>
    <li>
        <p>Upgrade worker nodes</p>
    </li>
</ol>
</p>

<blockquote>
<p>
One last thing before going to the steps. You are going to see that when we
<code>drain</code> a node we use the <code>--ignore-daemonsets</code> flag.
Which begs the question, what is a daemonset?
</p>
<p>
A daemonset defines pods needed for node-local stuff, say you want to have
a daemon on each node that collects logs. You can deploy a daemonset for it.
When we drain a node to upgrade we tell it to not kick out of there the
daemonsets, since we might actually need those for the node to operate
properly.
</p>
</blockquote>

<h4 id='upgrade_control_planes'><a href='#upgrade_control_planes'>Upgrade Control Planes</a></h4>
<ol>
    <li>
        <p><code>ssh</code> into the node</p>
    </li>
    <li>
        <p><code>k get nodes</code> to check current version</p>
    </li>
    <li>
        <p>
            Use your package manager <code>apt</code>/<code>dnf</code> and
            upgrade <code>kubeadm</code>
        </p>
    </li>
    <li>
        <p>
            Check which <code>kubeadm</code> versions are available to upgrade
            to
            <pre>
            <code>
$ sudo kubeadm upgrade plan
...
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.18.20
[upgrade/versions] kubeadm version: v1.19.0
I0708 17:32:53.037895   17430 version.go:252] remote version is much newer: \
v1.21.2; falling back to: stable-1.19
[upgrade/versions] Latest stable version: v1.19.12
[upgrade/versions] Latest version in the v1.18 series: v1.18.20
...
You can now apply the upgrade by executing the following command:

	kubeadm upgrade apply v1.19.12

Note: Before you can perform this upgrade, you have to update kubeadm to v1.19.12.
...
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p>Upgrade it <code>kubeadm upgrade apply v1.19.12</code></p>
    </li>
    <li>
        <p>
            Then we need to <u>drain</u> the node. Which means we mark the node
            as unschedulable, and new pods wont arrive.
            <pre>
            <code>
kubectl drain kube-control-plane --ignore-daemonsets
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p>
            Use your package manager to upgrade both <code>kubelet</code> and
            <code>kubectl</code> to the same version
        </p>
    </li>
    <li>
        <p>
            Restart and reload <code>kubelet</code> daemon with
            <code>systemctl</code>
        </p>
    </li>
    <li>
        <p>
            Mark node as schedulable again
            <pre>
            <code>
k uncordon kube-control-plane
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p><code>k get nodes</code> should show the new version</p>
    </li>
</ol>

<h4 id='upgrade_workers'><a href='#upgrade_workers'>Upgrade Workers</a></h4>

<ol>
    <li>
        <p><code>ssh</code> into the node</p>
    </li>
    <li>
        <p><code>k get nodes</code> to check current version</p>
    </li>
    <li>
        <p>
            Use your package manager <code>apt</code>/<code>dnf</code> and
            upgrade <code>kubeadm</code>
        </p>
    </li>
    <li>
        <p>
            Do <code>kubeadm upgrade node</code> to upgrade the
            <code>kubelet</code> configuration
        </p>
    </li>
    <li>
        <p>
            Drain the node as we did with the control plane
            <pre>
            <code>
kubectl drain worker-node --ignore-daemonsets
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p>
            Use your package manager to upgrade both <code>kubelet</code> and
            <code>kubectl</code> to the same version
        </p>
    </li>
    <li>
        <p>
            Restart and reload <code>kubelet</code> daemon with
            <code>systemctl</code>
        </p>
    </li>
    <li>
        <p>
            Mark node as schedulable again
            <pre>
            <code>
k uncordon worker-node
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p><code>k get nodes</code> should show the new version</p>
    </li>
</ol>
</section>

<h2 id='etcd'><a href='#etcd'>etcd</a></h2>
<section>
<p>
etcd is a key-value store used as k8s backing store for all the cluster
information. They are a stand alone project with its own <a
href="https://etcd.io/">docs</a>. Since it is used for backup, we need to know
how to use it in order to restore or backup the cluster.
</p>

<p>
There are two cli's we will be working with <code>etcdcutl</code> and
<code>etcdutl</code>.
<ul>
    <li>
        <p>
            <code>etcdctl</code>: primary way to interact with etcd over the
            network.
        </p>
    </li>
    <li>
        <p>
            <code>etcdutl</code>: designed to operate with etcd data files
            directly, not over the network.
        </p>
    </li>
</ul>
</p>

<p>
<code>kubeadm</code> will setup etcd as pods managed directly by the kubelet
daemon (known as <i>static pods</i>). You can actually see them by runnin g
</p>

<h3 id='backup_etcd'><a href='#backup_etcd'>Backing up etcd cluster</a></h3>
<p>
All k8s data is stored in etcd, this includes sensitive data, therefore the
snapshots created by etcd are encrypted.
</p>

<p>
In order to talk to etcd we can <code>ssh</code> into the control plane, then
do <code>etcdctl version</code> to verify it is installed.
</p>

<p>
If you went with <code>kubeadm</code> as your installation way, you can see
that there is a pod in the <code>kube-system</code> namespace that concerns
etcd. If you <code>describe</code> it you will some information relevant to
connect to etcd.
<pre>
<code>
k describe pod etcd-cka-control-plane -n kube-system | grep '\-\-'
      --listen-client-urls=https://10.2.0.9:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
</code>
</pre>

</p>
If  we want to talk to etcd from outside the control plane node, we will need
the <code>--listen-client-urls</code> addresses. If you are inside the node,
you can skip that. We are going to need the path to all the TLS things. A
simple command you can test if you have everything right is the following
<pre>
<code>
ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
    member list
    bbf4baa696b33a2e, started, control-plane, https://10.2.0.9:2380, https://10.2.0.9:2379
</code>
</pre>
Since the certificates are inside a path which your user probably does not have
access, you will have to <code>sudo</code> it.
</p>

<p>
The you can create an snapshot by running the <code>snapshot save
/path/to/new/snapshot</code> command.
<pre>
<code>
ETCDCTL_API=3 etcdctl --endpoints https://162.243.29.89:2379 \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt  \
    snapshot save snapshot.db
2025-08-09 17:04:29.201951 I | clientv3: opened snapshot stream; downloading
2025-08-09 17:04:29.241278 I | clientv3: completed snapshot read; closing
Snapshot saved at snapshot.db
</code>
</pre>
</p>

<h3 id='restore_etcd'><a href='#restore_etcd'>Restore from snapshot</a></h3>

<p>
We will use the <code>etcdutl</code> to restore a snapshot.
<pre>
<code>
etcdutl --data-dir /path/to/be/restored/to snapshot restore snapshot.db
</code>
</pre>
</p>

<p>
We also need to point the etcd pod to this new path we have restored the info
to. You can find the manifest for the etcd pod under
<code>/etc/kubernetes/manifests/etcd.yaml</code>. There is a volume called
<code>etcd-data</code>, point it to the new path, and restart the pod
</p>

</section>

<hr>
<h3 id='table'>~ Table of Contents</h3>
<ul>
  <li><a href='#exam_details'>Exam Details</a></li>
  <li>
    <a href='#k8s_nutshell'>K8s in a Nutshell</a>
    <ul>
      <li><a href='#features'>Features</a></li>
      <li><a href='#high_level_arch'>High-Level Architecture</a></li>
      <li><a href='#control_plane_nodes'>Control Plane Nodes</a></li>
      <li><a href='#shared_components'>Components Shared by Nodes</a></li>
      <li><a href='#advantages'>Advantages of Using k8s</a></li>
    </ul>
  </li>
  <li>
    <a href='#interacting_k8s'>Interacting with K8s</a>
    <ul>
      <li><a href='#api_primitives'>API Primitives and Objects</a></li>
      <li><a href='#kubectl'>kubectl</a></li>
      <li>
        <a href='#managing_objects'>Managing Objects</a>
        <ul>
          <li><a href='#imperative'>Imperative</a></li>
          <li><a href='#declarative'>Declarative</a></li>
          <li><a href='#hybrid'>Hybrid</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <a href='#cluster_installation'>Cluster Installation and Upgrade</a>
    <ul>
      <li><a href='#provision_infra'>Get Infrastructure Ready</a></li>
      <li><a href='#extension_interfaces'>Extension Interfaces</a></li>
      <li><a href='#setup_cluster'>Setup Cluster</a></li>
      <li><a href='#highly_available'>Highly Available (HA) Cluster</a>
        <ul>
          <li><a href="stacked_etcd">Stacked etcd topology</a></li>
          <li><a href="external_etcd">External etcd topology</a></li>
        </ul>
      </li>
      <li><a href='#upgrading_cluster'>Upgrading Cluster Version</a>
        <ul>
          <li><a href="#upgrade_control_planes">Upgrade Control Planes</a></li>
          <li><a href="#upgrade_workers">Upgrade Workers</a></li>
        </ul>
      </li>
  </li>
  <li>
    <a href='#etcd'>etcd</a>
        <ul>
          <li><a href="#backup_etcd">Backing up and etcd cluster</a></li>
          <li><a href="#restore_etcd">Restore from snapshot</a></li>
        </ul>
  </li>
</ul>
<p><a href='#top'>↑ go to the top</a></p>

</body>
</html>
