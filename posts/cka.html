<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" href="/static/water.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="UTF-8">
    <title>cka prep</title>
    <link rel="stylesheet" href="/static/highlight.min.css">
    <script src="/static/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <style>
        body {
            font-family: "SF Pro Display", system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue";
            max-width: 960px;
            margin: 0 auto;
        }
        h2 > a {
            color: black;
            text-decoration: none;
        }
        h2 > a:hover {
            text-decoration: underline;
        }
        h2 > a:visited {
            color: black;
        }

        a:visited {
            color: blue;
        }

        .important {
            background-color: lightgray;
            padding: 1em
        }

        .chapter {
            font-style: italic;

        }
    </style>
</head>
<body>
<a href='/'>
    < back home
</a>
<hr>
<h1 id='top'>~ cka prep</h1>
<hr>
<a href='#table'>Table of Contents</a>
<h2 class='chapter' id='exam_details'><a href='#exam_details'>1. Exam Details</a></h2>

<ul>
    <li>
        25% Cluster Arch, Installation and Configuration
        <ul>
            <li>
                Demonstrate the process of installing a cluster from scratch,
                upgrading a cluster version, and backing up restoring etcd
                database.
            </li>
        <li>
            Also RBAC
        </li>
        </ul>
    </li>
    <li>
        15% Workloads and Scheduling
        <ul>
        <li>
            Understand the effect of Pod resource limits on scheduling.
        </li>
        </ul>
    </li>
    <li>
        20% Services and Networking
        <ul>
        <li>
            Knowledge of Service and Ingress.
        </li>
        </ul>
    </li>
    <li>
        10% Storage
        <ul>
        <li>
            Volumes
        </li>
        </ul>
    </li>
    <li>
        30% Troubleshooting
        <ul>
        <li>
            Confronted with typical scenarios that you need to fix.
        </li>
        </ul>
    </li>
</ul>
<p>
It is essential that you become one with <i>`kubectl`</i>, <i>`kubeadm`</i>, and
<i>`etcdctl`</i>
</p>

<p>
When killing objects you may want to not wait for it to be shut down
gracefully, you can use <i>`--force`</i> to kill the object.
</p>

<h2 id='cluster_arch' class='chapter'> <a href='#cluster_arch'>2. Cluster Architecture, Installation and Configuration </a></h2>

<p>
Typical tasks you would expect a k8s admin to know, understanding the
architectural components, setting up a cluster from scratch, and maintaining a
cluster going forward
</p>

<h3 >Role-Based Access Control (RBAC) </h3>

<p>
To control who can access what on the cluster we need to establish certain
policies
</p>

<p>
RBAC defines policies for users, groups and processes, by letting them (or
disallowing them) to manage certain API resources.
</p>

<p>
RBAC has 3 building blocks:
</p>
<ul>
    <li>
        <b>Subject</b>
        <ul>
            <li>
                The user or process that want to access a resource
            </li>
            <li>
                E.g. Group, Users, SA (Service Accounts)
            </li>
        </ul>
    </li>
    <li>
        <b>Resource</b>
        <ul>
            <li>
                The k8s API object
            </li>
            <li>
                E.g. pods, deployments, services, etc..
            </li>
        </ul>
    </li>
    <li>
        <b>Verb</b>
        <ul>
            <li>
                The operation that can be executed on the resource.
            </li>
            <li>
                E.g. create, list, watch
            </li>
        </ul>
    </li>
</ul>



<h3> Creating a Subject </h3>

<p>
    Users and groups are not stored in <i>`etcd`</i> (the k8s db), they are
    meant for processes running outside of the cluster. On the other hand,
    service accounts are k8s objects and are used by processes running inside
    of the cluster.
</p>


<h4> User accounts and groups</h4>

<p>
    As stated before there is no k8s object for user, instead the job of
    creating the credential and distributing to the users is done externally by
    the admin.
</p>

<p>
    There are different ways k8s can authenticate a user:
</p>


<ul>
    <li>
        <b>X.508 client certificate</b>
        <ul>
            <li>Uses OpenSSL cert</li>
        </ul>
    </li>
    <li>
        <b>Basic Auth</b>
        <ul>
            <li>Uses username and password</li>
        </ul>
    </li>
    <li>
        <b>Bearer tokens</b>
        <ul>
            <li>Uses OpenID or webhooks as auth</li>
        </ul>
    </li>
</ul>

<p>
You wont have to create a user in the exam, but here are the steps for creating
one using the OpenSSL method.
</p>

<ol>
    <li>
        <p>
            Go to k8s control plane node and create a temporary dir that will
            have the keys.
        </p>
        <code>mkdir cert && cd cert</code>
    </li>
    <li>
        <p>Create a private key using <i>openssl</i> (username.key)</p>
        <code>openssl genrsa -out johndoe.key 2048</code>
    </li>
    <li>
        <p>Create a cerficiate sign request (a .csr file) with the key from the previous step</p>
        <code>
            openssl req -new -key johndoe.key -out johndoe.csr -subj
                "/CN=johndoe/O=cka-study-guide"
        </code>
    </li>
    <li>
        <p>Sign the .csr with the k8s CA (it usually is under <i>/etc/kubernetes/pki</i>)</p>
        <code>
            openssl x509 -req -in johndoe.csr -CA ca.crt -CAkey ca.key
            -CAcreateserial -out johndoe.crt -days 364
        </code>
    </li>
    <li>
        <p>Add it to your kubeconfig file</p>
        <code>
            kubectl config set-credentials johndoe
            --client-certificate=johndoe.crt --client-key=johndoe.key
        </code>
    </li>
</ol>

<h4> ServiceAccount </h4>

<p>
The users we created in the last paragraphs are meant to be used by humans, if
a pod or a <i>svc</i> needs to authenticate against the k8s cluster we need to
create a service account.
</p>

<p>
A k8s cluster already comes with a <i>`sa`</i> called default. Any pod that
does not explicitly assign a service account uses the default service account
</p>

<p>
It is super simple to create one with the imperative approach
</p>
<p>
<code>
$ k create sa build-bot
</code>
</p>

<p>
When creating a service account a secret holding the API token will be created
too. The Secret and token names use the Service Account name as a prefix.
</p>

<p>
To assing a service account to a pod you can do it imperatively by:
</p>
<code>
$ k run build-observer --image=alpine --restart=Never --serviceaccount=build-bot
</code>
</p>
<p>
Or add it to the manifest under <i>`spec.serviceAccountName`</i>.
</p>
<p class='important'>
 <b>vvv</b> One more important thing. <b>vvv</b>
</p>
<p>
If you want to make a call to k8s api from within a pod using
your <i>serviceaccount</i>. You will need to create a token, and then do the
requests using the internal dns.
</p>
<ul>
  <li>
    <i> k create token SERVICE_ACCOUNT_NAME </i>
  </li>
  <li>
    <p>
        <i> k exec -it mypod -- /bin/bash</i>
    </p>
    <p>
        <i> # curl -k -H "Authorization: Bearer the_token_you_just_got" https://kubernetes.default.svc/api/v1/namespaces</i>
    </p>
  </li>
</ul>

<h3> Roles and RoleBindings </h3>

<p>
We have these two primitives:
</p>

<ul>
    <li>
        <b>Role</b>
        <p>Declares the api resources, and their operations. E.g. <i>Allow listing and deleting pods</i></p>
    </li>
    <li>
        <b>RoleBinding</b>
        <p>Connects or binds the roles to a <i>Subject</i></p>
    </li>
</ul>


<p>
There are some default roles:
</p>
<ul>
    <li>
        <b>cluster-admin</b>
        <p>rw access to resources across all namespaces</p>
    </li>
    <li>
        <b>admin</b>
        <p>
            rw access to resources in namespaces including roles and
            rolebinding
        </p>
    </li>
    <li>
        <b>edit</b>
        <p>
            rw access to resources in namespace except roles and rolebindings.
        </p>
    </li>
    <li>
        <b>view</b>
        <p>
            ro access to resources in namespace except roles, rolebindings and
            secrets.
        </p>
    </li>
</ul>

<h4>Creating Roles and RoleBindings</h4>

<p>
Imperative mode for roles.
</p>

<code>
k create role read-only --verb=list,get,watch --resource=pods,deployments,services
</code>

<p>
There is also <i>`--resource-name`</i> as a flag, where you can specify names
of pods.
</p>


<p>
Imperative mode for rolebindings.
</p>

<code>
$ k create rolebinding read-only-binding --role=read-only --user=johndoe
</code>

<p>
If then you do a <i>`get`</i> you wont see the subject, you need to render the
details to see that.
</p>

<p>
You can of course use <i>`describe`</i> and so on to check each of the
primitives once created, but there is also this little neat command: <i>$ k
auth can-i</i> this will give specific info on user permissions </p>

<p>
<code>
$ kubectl auth can-i --list --as johndoe
</code>
</p>
<p>
<code>
$ kubectl auth can-i list pods --as johndoe
</code>
</p>
<p>
<code>
Yes
</code>
</p>

<h3>Cluster Role</h3>

<p>
    Roles and RoleBinding are namespace specific. If we want to define the same
    but for the whole cluster we need to use <i>ClusterRole</i> and
    <i>ClusterRoleBinding</i>. The configuration elements are the same.
</p>

<h3>Aggregating RBAC Rules</h3>

<p>
Finally, we can aggregate different roles with label selection. Say you have
one role that lets you list pods, and other that lets you delete pods.
</p>

<p>
You can aggregate them with an <i>`aggregationRule`</i>
</p>

<p>
Here is an example from <b>Benjamin Muschko's Oreilly's Certified Kubernetes
Administrator (CKA) Study Guide.</b>
</p>

<p>
  YAML manifest for listing pods
</p>
<pre>
<code>
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: list-pods
  namespace: rbac-example
  labels:
    rbac-pod-list: "true"
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
</code>
</pre>

<p>
  YAML manifest for deleting svc's
</p>
<pre>
<code>
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: delete-services
  namespace: rbac-example
  labels:
    rbac-service-delete: "true"
rules:
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - delete
</code>
</pre>

<p>
  YAML manifest aggregating them.
</p>
<pre>
<code>
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pods-services-aggregation-rules
  namespace: rbac-example
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac-pod-list: "true"
  - matchLabels:
      rbac-service-delete: "true"
rules: []
</code>
</pre>

<h2 id='creating_and_managing_k8s'><a href='#creating_and_managing_k8s'>Creating and Managing a K8s Cluset</a></h2>

<p>
Common tasks an admin is expected to perform are:
</p>

<ul>
    <li>bootstrapping a control plane node</li>
    <li>bootstrapping worker nodes and joining them to the cluster</li>
    <li>upgrading a cluster to a newer version</li>
</ul>

<p>
For bootstrapping oprations we use <i>`kubeadm`</i>. You will need to provision
the underlying infra before, using ansible or terraform.
</p>

<p>
For the exam you can expect <i>`kubeadm`</i> to be installed already.
</p>

<p>
To start a cluster basically on your master machine, you need on have a
container runtime up and running, such as <i>`containerd`</i>. Then you follow
some steps:
<ul>
  <li> Install <a href='https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/'>kubeadm</a> on the nodes. </li>
  <li>
    On master run
    <pre>
<code>
sudo kubeadm init --pod-network-cidr 172.18.0.0/16
</code>
    </pre>
  </li>
  <li>
    Check all the logs that were printed. From there you can copy the snippet
    on getting your kubeconfig file.
    <pre>
<code>
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code>
    </pre>
  </li>
  <li>
    Keep reading the logs, from there you will also be able to get, the command to join the workers.
    <pre>
<code>
kubeadm join "some_ip" --token some_token \
  --discovery-token-ca-cert-hash some:cert
</code>
    </pre>
  </li>
  <li>
    <p>
        Finally we need to deploy a Container network interface so pods can
        talk to each other. There are a lot of options, Flannel, Calico, and so
        on.
    </p>
    <p>
        At the end of the day, whichever your choose, probably you will  only
        have to  run a <i>`k apply -f`</i> command.
    </p>
  </li>
  <li>
    Verify everything by doing <i>`k get nodes`</i>.
  </li>
</ul>
</p>
<h2 id='highly_avaliable'>Highly Available Cluster</h2>
<p>
    If we have a cluster, with just one node as a control plane, it can become
    risky. If the control plane node dies, we are not going to be able to talk
    with k8s API; neither the workers.
</p>
<p>
    There is this concept of High-availability (HA) clusters, these help with
    scalability and redundancy.  Due to the complexity of setting them up, it
    is not likely you are going to have to perform the steps during the exam.
    But this talks about the idea of having multiple control planes nodes.
    There are different architectures, here I will briefly explain some.
</p>
<h3>Stacked etcd topology</h3>
<p>
    This involves creating two or more control plane nodes where etcd is
    located <b>inside</b> the node.  These will talk to the workers via a load
    balancer.
</p>
<h3>External etcd topology</h3>
<p>
    This involves creating two or more control plane nodes where etcd is
    located <b>outside</b> the node. These will talk to the workers via a load
    balancer. Do note that for these you need a extra node per control plane.
    Since we will have an <i>etcd</i> node per control node.
</p>

<p>
    The main difference is that etcd is outside the control nodes, so if a
    control node fails, etcd will still be there.
</p>

<h2 id='upgrade_k8s'>Upgrade k8s version</h2>

<p>
   Only upgrade from a minor version to a next higher one. <i>1.18 -> 1.19
   </i>, or from  a patch version to a higher one. <i>1.18.0 -> 1.18.3</i>.
   Abstain from jumping up multiple minor versions, to avoid unexpected side
   effects.
</p>

<h3>Upgrade Control Planes</h3>
<p>
    If you are managing a high available cluster, you need to upgrade one
    control plane node at a time.
</p>
<ol>
    <li>
        <p>ssh to the control plane</p>
    </li>
    <li>
        <p>get current version, you can do <i>`k get nodes`</i></p>
    </li>
    <li>
        <p>
            upgrade <i>kubeadm</i> to the version you want with your package
            manager.
        </p>
        <pre>
<code>
sudo apt-get install -y kubeadm=1.19.0-00
</code>
        </pre>
    </li>
    <li>
        <p>Check which versions are available to upgrade to</p>
        <pre>
<code>
$ sudo kubeadm upgrade plan
...
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.18.20
[upgrade/versions] kubeadm version: v1.19.0
I0708 17:32:53.037895   17430 version.go:252] remote version is much newer: \
v1.21.2; falling back to: stable-1.19
[upgrade/versions] Latest stable version: v1.19.12
[upgrade/versions] Latest version in the v1.18 series: v1.18.20
...
You can now apply the upgrade by executing the following command:

	kubeadm upgrade apply v1.19.12

Note: Before you can perform this upgrade, you have to update kubeadm to v1.19.12.
...
</code>
        </pre>
    </li>
    <li>
        <p> upgrade it <i>`sudo kubeadm upgrade apply v1.19.0`</i></p>
    </li>
    <li>
        <p>we need to <u>drain</u> the node.</p>
        <p>
            if the concept of drain is new to you, you are not alone, it
            basically means, that the given node will be marked unschedulable
            to prevent new pods from arriving.
        </p>
        <pre>
<code>
kubectl drain kube-control-plane --ignore-daemonsets
</code>
        </pre>

    </li>
    <li>
        <p>
            Upgrade <i>kubelet</i> and <i>kubectl</i> to the same version.
            Again, using your package manager.
        </p>
    </li>
    <li>
        <p>Restart and reaload <i>`kubelet`</i> process using <i>systemclt</i></p>
    </li>
    <li>
        <p>Mark node as schedulable again</p>
        <p><i>`k uncordon kube-control-plane`</i></p>
    </li>
    <li>
        <p>
            If you do <i>`k get nodes`</i> again now you should see that node
            with the new version.
        </p>
    </li>
</ol>


<h3>Upgrade Worker nodes</h3>

<ol>
    <li>
        <p>ssh to the node</p>
    </li>
    <li>
        <p>upgrade <i>kubeadm</i> using your package manager</p>
    </li>
    <li>
        <p>upgrade the node using <i>kubeadm</i></p>
        <pre>
<code>
sudo kubeadm upgrade node
</code>
        </pre>
    </li>
    <li>
        <p>we need to <u>drain</u> (make it unscheduable) the node.</p>
        <pre>
<code>
kubectl drain worker-node --ignore-daemonsets
</code>
        </pre>
    </li>
    <li>
        <p>
            Upgrade <i>kubelet</i> and <i>kubectl</i> to the same version.
        </p>
    </li>
    <li>
        <p>Restart and reaload <i>`kubelet`</i> process using <i>systemclt</i></p>
    </li>
    <li>
        <p>Mark node as schedulable again</p>
        <p><i>`k uncordon worker-node`</i></p>
    </li>
    <li>
        <p>
            If you do <i>`k get nodes`</i> again now you should see that node
            with the new version.
        </p>
    </li>

</ol>

<h2 class='etcd_backup'>Backing up and Restoring etcd</h2>

<p>
<i>`etcd`</i> is where k8s stores both the declared and observed states of the
cluster. It uses a distributed key-value store.
</p>

<p>
It is important to have a backup, in case of some issue. The backup process
should happen periodically  and in short time frames.
</p>

<p>
We will use the <i>etcdctl</i> tool to do this.:
</p>
<h3>Backup Process</h3>
<ol>
    <li>
        <p>ssh into the control pane</p>
    </li>
    <li>
        <p>check you have a higher than 3.4 etcdctl version installed</p>
    </li>
    <li>
        <p>Get and describe the pod etcd is running</p>
        <p><i>`k get pods -n kube-system`</i><p>
        <p><i>`k describe pod etcd-smth l-n kube-system`</i><p>
    </li>
    <li>
        <p>
            Look for the value under <i>--listen-client-urls</i>. We are going
            to need to the path for the <i>server.crt</i>, <i>.key</i>, and the
            <i>ca.crt</i>.
        </p>
        <pre>
<code>
$ sudo ETCDCTL_API=3 etcdctl
  --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> \
  snapshot save <backup-file-location>
</code>
        </pre>
        <p>
            It would look something like this:
        </p>
        <pre>
<code>
sudo ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/etcd-backup.db
</code>
        </pre>
    </li>
    <li>
        <p>Store the backup in a safe place</p>
    </li>
</ol>
<h3>Restore Process</h3>
<ol>
    <li>
        <p>ssh into the system</p>
    </li>
    <li>
        <p>run the <i>`snapshot restore`</i> command</p>
        <pre>
<code>
sudo ETCDCTL_API=3 etcdctl --data-dir=/var/lib/from-backup snapshot restore \
  /opt/etcd-backup.db
</code>
        </pre>
    </li>
    <li>
        <p>Now we need to change the etcd manifest, to point to our backup</p>
        <p>It is mounted as a volume, with the <i>`etcd-data`</i> name.</p>
        <p>
            These manifests are declared under
            <i>/etc/kubernetes/manifests</i>. The pod will be re-created
            (automatically?) pointing to the backup directory
        </p>
    </li>
</ol>
<h2 class='essentials_cluster_arch'>Essentials for Exam</h2>
<ul>
    <li>
        <p>Know how to define RBAC rules</p>
        <ul>
            <li>
                Practice the creation of subjects, and how to tie them together
                to form the desired access rules.
            </li>
        </ul>
    </li>
    <li>
        <p>Know how to create and manage a k8s cluster</p>
        <ul>
            <li>
                Installing new cluster nodes
            </li>
            <li>
                Understand the different HA topologies.
            </li>
        </ul>
    </li>
    <li>
        <p>Practice backing up and restoring etcd</p>
    </li>
</ul>

<h2 id='troubleshooting' class='chapter'><a href='#troubleshooting'>3. Troubleshooting</a></h2>
<h3 id='cluster_logging'>Cluster Logging</h3>
<p>
Since we will have a node logs, pods logs, container logs it makes sense to
think about how we will handle these streams. In a cluster, logs should have a
separate storage and lifecycle independent of nodes, pods or containers.   This
process is called <i>cluster-level logging.</i>
</p>

<p>
One other thing worth mentioning, <b>log rotation</b>, the kubelet daemon is
responsible for rotating container logs and managing the logging directory
structure. It tells the container runtime where to write container logs.
</p>

<p>
You can configure <i>`containerLogMaxSize`</i> (max size of file) and
<i>`containerLogMaxFiles`</i> (how many log files can be created)
</p>

<p>
So for Cluster-Level logging we have these options:
</p>

<ul>
  <li>
    <p>Use a node-level logging agent</p>
    <ul>
      <li>
        <p>
            Create a <i>DeamonSet</i> that will act as an agent per node.  It
            will look inside a directory where the logs are stored and push
            them to a logging backend.
        </p>
      </li>
    </ul>
  </li>
  <li>
    <p>Include dedicated sidecar container for handling logs in a pod</p>
  </li>
  <li>
    <p>
        Push the logs directly from the app to a back-end (you have to modify
        the app code itself)
    </p>
  </li>
</ul>

<h3 id='troubleshooting_pods'>Troubleshooting Pods</h3>
<p>
Creating a Pod is pretty simple, as long as the yaml is right k8s will try to
create it. Nevertheless we need to verify the correct behaviour. The first
thing is to verify the high-level runtime information of the Pod.
</p>

<p>
Check the resources, the deployment, the pods, check the status column. Check
the number of restarts.
</p>
<p class='.important'>
If the number of restarts is greater than 0, then you might want to check the
logic of the liveness probe. Identify why the restart was necessary.
</p>

<p>
Here are some of the common status errors that one might come across.
</p>
<ul>
  <li>
    <p>
      <i>ImagePullBackOff</i> or <i>ErrImagePull</i>: check correct image name,
      check if image exists in registry, verify network access from node to
      registry, check auth.
    </p>
  </li>
  <li>
    <p>
      <i>CrashLoopBackOff</i>: this means the application or command run in
      container crashes. Check the command executed. Make sure that the
      container can be created. Like run it with podman or smth
    </p>
  </li>
  <li>
    <p>
      <i>CreateContainerConfigError</i>: the configmap or secret referenced by
      the container cannot be found. Double check if the objects exists in the
      namespace.
    </p>
  </li>
</ul>

<p>
So a quick list on what to do:
</p>
<ul>
  <li>
    <p>Check status</p>
  </li>
  <li>
    <p>Check events</p>
  </li>
  <li>
    <p>Inspect the logs of the pod</p>
  </li>
  <li>
    <p>
      If nothing else works, exec into a pods container, see if everything is
      behaving like it should
    </p>
  </li>
</ul>

<h3 id='troubleshooting_services'>Troubleshooting Services</h3>
<ul>
  <li>
    <p>Ensure the label selector matches the assigned labels of the Pods</p>
    <ul>
      <li>
        <p>
          A really good command for checking this is the <i>`k get
          endpoints`</i> command. It will tell you which pods is your service
          using.
        </p>
      </li>
    </ul>
  </li>
  <li>
    <p>Check the type of a Service.</p>
    <ul>
      <li>
        <p>
            The default is <i>`ClusterIP`</i>, if so only the pods within that
            node can talk to the service.
        </p>
      </li>
    </ul>
  </li>
  <li>
    <p>
        Check the port mapping, meaning check that the service target port is
        the same as the port exposed by the pod.
    </p>
    <ul>
      <li>
        <p>
            The default is <i>`ClusterIP`</i>, if so only the pods within that
            node can talk to the service.
        </p>
      </li>
    </ul>
  </li>
</ul>



<hr>
<h3 id='table'>~ Table of Contents</h3>
<ul>
    <li><a href='#exam_details'>Exam Details</a></li>
    <li>
        <a href='#cluster_arch'>Cluster Architecture, Installation and Configuration </a>
        <ul>
            <li><a href='#creating_and_managing_k8s'>Creating and Managing a K8s Cluster</a></li>
            <li><a href='#highly_avaliable'>Highly Available Cluster</a></li>
            <li><a href='#upgrade_k8s'>Upgrade k8s Cluster</a></li>
            <li><a href='#etcd_backup'>Backing up and Restoring etcd</a></li>
            <li><a href='#essentials_cluster_arch'>Essentials</a></li>
        </ul>
    </li>
    <li>
        <a href='#troubleshooting'>Troubleshooting</a>
        <ul>
            <li><a href='#cluster_logging'>Cluster Logging</a></li>
            <li><a href='#troubleshooting_pods'>Troubleshooting Pods</a></li>
        </ul>
    </li>
</ul>
<p><a href='#top'>↑ go to the top</a></p>
<ul>


</body>
</html>


