<!DOCTYPE html>
<html lang="en">
<head>
  <link rel="stylesheet" href="/static/water.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">
  <title>cka prep</title>
  <link rel="stylesheet" href="/static/highlight.min.css">
  <script src="/static/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>

    body {
      font-family: open sans, "SF Pro Display", system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue";
      max-width: 960px;
      margin: 0 auto;
    }
    h1 > a, h2 > a, h3 > a, h4 > a, h5 > a, h6 > a {
      color: black;
      text-decoration: none;
    }
    h1 > a:hover, h2 > a:hover, h3 > a:hover, h4 > a:hover, h5 > a:hover, h6 > a:hover {
      text-decoration: underline;
    }
    h1 > a:visited, h2 > a:visited, h3 > a:visited, h4 > a:visited, h5 > a:visited, h6 > a:visited {
      color: black;
    }

    a:visited {
      color: blue;
    }

    .important {
      background-color: lightgray;
      padding: 1em
    }

    .chapter {
      font-style: italic;

    }

    li > details {
        margin: 0;
        padding: 0;
        background-color: white;
    }
    li > details[open] {
        margin: 0;
        padding: 0;
        background-color: white;
    }
    li > details > * {
        margin: 0;
        padding-top: 0;
        background-color: white;
    }
    summary {
        background-color: white;
        padding: 0;
        margin: 0;
    }
    summary > * {
        margin: 0;
        padding: 0;
    }
  </style>
</head>
<body>
<a href='/'>
  < back home
</a>
<hr>
<small><i>
Crated on <time datetime="2025-07-18">July 18, 2025</time>; Last modified on
<time datetime="2025-08-24">Aug 24, 2025.</time>
</i></small>
<br>
<small>
This used to be a different page. Go to <a href="/posts/cka-old.html">cka old
notes</a> to check those out.
</small>

<h1 id='top'>~ cka prep</h1>
<a href='#table'>Table of Contents</a>

<h2 id='exam_details'><a href='#exam_details'>Sections</a></h2>
<section>
<ul>
  <li> 25% Cluster Arch, Installation and Configuration </li>
  <li> 15% Workloads and Scheduling </li>
  <li> 20% Services and Networking </li>
  <li> 10% Storage </li>
  <li> 30% Troubleshooting </li>
</ul>
</section>

<hr>
<h2 id='k8s_nutshell'><a href='#k8s_nutshell'>K8s in a Nutshell</a></h2>
<section>
<p>
K8s is a container orchestration tool. Today there are a lot of microservices
architectures. We can have a container per service and managing all those
containers can be hard. K8s takes care of the scalability, security,
persistence and load balancing.
</p>
<p>
When k8s is triggered to create a container, it will delegate it to the
container runtime engine via a CRI (container runtime interface).
</p>

<h3 id='features'><a href='#features'>Features</a></h3>
<ul>
  <li>
    <b>Declarative Model</b>
    <ul>
      <li>
        the cool thing about k8s, is that you just tell it the status
        of the cluster you want, via yamls, and it will do its best to
        create it.
      </li>
    </ul>
  </li>
  <li>
    <b>Autoscaling</b>
    <ul>
      <li>
        k8s can also automatically (or manually) scale resources when
        needed.
      </li>
    </ul>
  </li>
  <li>
    <b>Application management</b>
    <ul>
      <li>
        When deploying new versions for your application, you can use
        k8s to manage the roll out technique. As well as come back to
        previous rollouts
      </li>
    </ul>
  </li>
  <li>
    <b>Persistent Storage</b>
    <ul>
      <li>
        Containers are ephemeral, meaning the filesystem in them will
        die with them. With k8s you can have persistent volumes, to
        manage storage across containers.
      </li>
    </ul>
  </li>
  <li>
    <b>Networking</b>
    <ul>
      <li>
        k8s has internal and external load balancing for network
        traffic.
      </li>
    </ul>
  </li>
</ul>

<h3 id='high_level_arch'><a href='#high_level_arch'>High-Level Architecture</a></h3>
<p>
There are two types of nodes (these can be vm, baremetal machines, whatever
you call a computer):
</p>
<ul>
  <li>
    <b>Control plane nodes</b>
    <ul>
      <li>
        This node exposes k8s API thru a server. Whenever you do
        <code>kubectl something</code> you are talking to this API.
      </li>
    </ul>
  </li>
  <li>
    <b>Worker nodes</b>
    <ul>
      <li>
        These are the nodes that execute the workload in containers
        managed by pods. Worth noting that every worker node needs a
        <details>
        container runtime engine, to create the containers.
      </li>
    </ul>
  </li>
</ul>

<h3 id='control_plane_nodes'><a href='#control_plane_nodes'>Control Plane Nodes</a></h3>
<p>These have different components to do their job:</p>
<ul>
  <li>
    <b>API Server</b>
    <ul>
      <li>
        Exposes the k8s api to clients (kubectl)
      </li>
      <li>
        Here is where the authentication, authorization and admission
        control also happens.
      </li>
    </ul>
  </li>
  <li>
    </b>Scheduler</b>
    <ul>
      <li>
        Background process that watches for new k8s pods with no
        assigned and look for a node to execute them.
      </li>
    </ul>
  </li>
  <li>
    <b>Control Manager</b>
    <ul>
      <li>
        Watches the state of your clusters and implements changes where
        needed.
      </li>
      <li>
        This is the guy that makes the cluster be to its desired state.
      </li>
    </ul>
  </li>
  <li>
    <b>Etcd</b>
    <ul>
      <li>
        key-value db that stores all k8s cluster related data.
      </li>
    </ul>
  </li>
</ul>

<h3 id='shared_components'><a href='#shared_components'>Components Shared by Nodes</a></h3>
<p>
There are some components that are shared by the nodes, whether they are
control or workers.
</p>
<ul>
  <li>
    <b>Kubelet</b>
    <ul>
      <li>
        agent that makes sure the necessary containers are running in a
        pod.
      </li>
      <li>
        this is the glue between k8s and the container runtime engine.
      </li>
    </ul>
  </li>
  <li>
    <b>Kube Proxy</b>
    <ul>
      <li>
        every node has this network proxy to enable network
        communication and implement their rules.
      </li>
    </ul>
  </li>
  <li>
    <b>Container runtime</b>
    <ul>
      <li>
        the container runtime that will manage the containers. Not
        really needed in control planes.
      </li>
    </ul>
  </li>
</ul>

<h3 id='advantages'><a href='#advantages'>Advantages on Using k8s</a></h3>
<ul>
  <li>
    <b>Portability</b>
    <ul>
      <li>
        the container runtime engine can run stuff regardless of whether
        you run it on a vm, bm or even on a raspberry. So you have
        portability for your things regardless of where the cluster is
        located.
      </li>
    </ul>
  </li>
  <li>
    <b>Resilience</b>
    <ul>
      <li>
        controllers are always looking to be on the desired state.
        Meaning it will try to self-heal when something goes array.
      </li>
    </ul>
  </li>
  <li>
    <b>Scalability</b>
    <ul>
      <li>
        k8s can scale the number of pods on demand or automatically
      </li>
    </ul>
  </li>
  <li>
    <b>Extensibility</b>
    <ul>
      <li>
        When the core functionality is not enough you can introduce CRDs
        that extend the functionality of the cluster.
      </li>
    </ul>
  </li>
</ul>
</section>

<hr>
<h2 id='interacting_k8s'><a href='#interacting_k8s'>Interacting with K8s</a></h2>
<section>

<h3 id='api_primitives'><a href='#api_primitives'>API Primitives and Objects</a></h3>
<p>
K8s has api resources which are the building blocks of the cluster. These are
your pods, deployments, services, and so on.
</p>
<p>Every k8s primitive follows a general structure:</p>
<ul>
  <li>
    <i>api version:</i>
    <ul>
      <li>
        defines the structure of a primitive and uses it to validate the
        correctness of the data.
      </li>
      <li>
        you can do <code>k api-versions</code> to see the versions
        compatible with your cluster
      </li>
    </ul>
  </li>
  <li>
    <i>kind</i>
    <ul>
      <li>
        the type of the primitive
      </li>
    </ul>
  </li>
  <li>
    <i>metadata</i>
    <ul>
      <li>
        name, namespace, high level info
      </li>
      <li>
        here you see the UID, which is an id k8s generates for each
        object.
      </li>
    </ul>
  </li>
  <li>
    <i>spec</i>
    <ul>
      <li>
        the desired state of the resource
      </li>
    </ul>
  </li>
  <li>
    <i>status</i>
    <ul>
      <li>
        actual state of the resource
      </li>
    </ul>
  </li>
</ul>

<h3 id='kubectl'><a href='#kubectl'>kubectl</a></h3>
<p>
This is how we talk to the api. Usually you do:
</p>
<pre><code>k &lt;verb&gt; &lt;resource&gt; &lt;name&gt;</code></pre>
<p>
Keep in mind we usually have different stuff in different namespaces, so we
are always appending <code>-n &lt;some namespace&gt;</code> to the command.
</p>
<p>
The name of an object has to be unique across all objects of the same resource
within a namespace.
</p>

<h3 id='managing_objects'><a href='#managing_objects'>Managing Objects</a></h3>
<p>
There are two ways to manage objects: the imperative or the declarative way.
</p>

<h4 id='imperative'><a href='#imperative'>Imperative</a></h4>
<p>
The imperative is where you use commands to make stuff happen in the cluster.
Say you want to create an nginx pod you would do:
</p>
<pre><code>k run --image=nginx:latest nginx --port=80</code></pre>
<p>
This would create the pod in the cluster when you hit enter. In my
professional experience, you hardly ever create stuff like that. The only
time I use it is to create temporary pods to test something.
</p>
<p>
There are other verbs which you might use a bit more. <code>edit</code> brings
up the raw config of the resource and you can change it on the fly. Although
I would recommend just do this for testing things. Hopefully your team has
the manifests under a version control system, if you edit stuff like this it
would mess it up.
</p>
<p>
There is also <code>patch</code> which I have never used, but it... "Update
fields of a resource using strategic merge patch, a JSON merge patch, or a
JSON patch."
</p>
<p>
There is also <code>delete</code> which -- as you probably guess already --
deletes the resource. Usually the object gets a 30 sec grace period for it to
die. But if it does not the kubelet will try to kill it forcefully.
</p>
<p>If you do:</p>
<pre><code>k delete pod nginx --now</code></pre>
<p>It will ignore the grace period.</p>

<h4 id='declarative'><a href='#declarative'>Declarative</a></h4>
<p>
This is where you have a bunch of <code>yaml</code>s which are your
definitions of resources. The cool thing about this is that you can version
control them. Say you have a <code>nginx-deploy.yaml</code>. You can create
it in the cluster with:
</p>
<pre><code>k apply -f nginx-deploy.yaml</code></pre>
<p>
This gives you more flexibility on what you are doing. Since you can just go
to the file change stuff and apply it again.
</p>

<h4 id='hybrid'><a href='#hybrid'>Hybrid</a></h4>
<p>
Usually I use a hybrid approach, most of the imperative commands have this
<code>--dry-run=client -o yaml</code> flag that you can append to the command
and it will render the yaml manifest. You can redirect that to a file and
start working on that. You open the yaml with your favourite text editor, and
then mount volumes and stuff like that.
</p>
<p>
There are more ways to manage the resources for example you can use kustomize
to render different values based on the same manifest, or with helm to bring
up complete apps/releases to just cluster. Probably we will go over them
later in the book.
</p>
</section>

<hr>
<h2 id='cluster_installation'><a href='#cluster_installation'>Cluster Installation and Upgrade</a></h2>
<section>

<h3 id='provision_infra'><a href='#provision_infra'>Get Infrastructure Ready</a></h3>

<p>
There are a million ways of doing this. I used terraform to create some
droplets in digital ocean and packer with ansible to build an image that would
let everything ready for me to run the <code>kubeadm</code> commands.
</p>

<p>
<code>kubeadm</code> is the tool to create a cluster.
</p>

<p>
Here is a non-comprehensive list of what is needed before running
<code>kubeadm</code> stuff.

<ul>
    <li>
        <p>
            Open <a
            href='https://kubernetes.io/docs/reference/networking/ports-and-protocols/'>ports
            needed</a> for k8s to work
        </p>
    </li>
    <li>
        <p>Disable swap; otherwise kubelet is going to fail to start</p>
    </li>
    <li>
        <p>Install a container runtime, like containerd</p>
    </li>
    <li>
        <p>Install <code>kubeadm</code></p>
    </li>
</ul>
</p>


<h3 id='extension_interfaces'><a href='#extension_interfaces'>Extension Interfaces</a></h3>
<p>
There are some things k8s does not have by default. You need to install this
extensions as needed.
<ul>
  <li>
    <p>
      Container Network Interface (CNI)
      <ul>
        <li>
            This manages the network interaction between containers.
        </li>
      </ul>
    </p>
  </li>
  <li>
    <p>
      Container Network Interface (CRI)
      <ul>
        <li>
            This is the piece that interacts with the Container Runtime
            (containerd, or other) to tell it what to do. Kill, create
            containers and so on.
        </li>
      </ul>
    </p>
  </li>
  <li>
    <p>
      Container Storage Interface (CSI)
      <ul>
        <li>
          This is the standard to implement plugins for interacting with
          block/file storage
        </li>
      </ul>
    </p>
  </li>
</ul>
</p>

<h3 id='setup_cluster'><a href='#setup_cluster'>Setup Cluster</a></h3>

<p>
Once you have <code>kubeadm</code> in your system everything else is pretty
straight forward. You just ssh to your control plane and run:
<pre>
<code>
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
</code>
</pre>
</p>

<p>
This runs some preflight checks to see if everything is working properly, if
not it will likely print a message telling you about what is wrong. In my case
it complained about <code>/proc/sys/net/ipv4/ip_forward</code> being disabled.
But was able to fix it by just doing <code>echo 1 | sudo tee
/proc/sys/net/ipv4/ip_forward</code>.
</p>

<p>
Where does the <code>cidr</code> comes from? I had exactly the same question.
It seems that it will depend on the CNF you will install, but do not quote me
on that.
</p>

<p>
Once the command runs successfully, it will print next steps:
<pre>
<code>
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join ip.control.panel.and:port --token some-cool-token \
    --discovery-token-ca-cert-hash sha256:some-cool-hash
</code>
</pre>
</p>

<p>
Just follow those steps. You ssh into the workers and join them with that
command. If you lost the tokens for some reason you can reprint them with:
<pre>
<code>
kubeadm token create --print-join-command
</code>
</pre>
</p>

<p>
Now, before joining the workers, you need to install the CNI, you can pick any
of the ones on <a
href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">the
k8s add-ons docs</a>.
</p>

<p>
Installing them is nothing fancy, your literally just run a `k apply -f
some-mainfest` and be done with it. I went with <a
href="https://docs.tigera.io/calico/latest/getting-started/kubernetes/flannel/install-for-flannel">calico</a>
for no particular reason.
</p>

<h3 id='highly_available'><a href='#highly_available'>Highly Available (HA) Cluster</a></h3>

<p>
The control plane is like the most important part of the cluster, since if it
fails, you are not even going to be able to talk to the API to do stuff. We can
add redundancy to improve this. Here is where HA architectures come into play.
</p>

<p>
There are two
<ul>
    <li>
        <p>Stacked etcd topology</p>
    </li>
    <li>
        <p>External etcd topology</p>
    </li>
</ul>
Remember <i>etcd</i> is key-value db k8s uses to store all of its info.
</p>

<h4 id='stacked_etcd'><a href='#stacked_etcd'>Stacked etcd topology</a></h4>
<p>
You have at least three control planes each with its own etcd in the same node.
All the nodes running at the same time and the workers talk to them through a
load balancer, if one dies, we still have others.
</p>
<img src="/static/img/cka/kubeadm-ha-topology-stacked-etcd.svg" alt="stacked etcd topology">
<small>
Diagram representing this arch, that I stole from the <a href="https://kubernetes.io/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg">k8s docs</a>
on this topic.
</small>
<hr>

<h4 id='external_etcd'><a href='#external_etcd'>External etcd topology</a></h4>
<p>
Per control plane we have two nodes, one that runs etcd and one that runs the
actual control plane stuff. They communicate through the
<code>kube-apiserver</code> api.
</p>
<p>
This topology require more nodes, and that means a bit more manage overhead.
</p>
<img src="/static/img/cka/kubeadm-ha-topology-external-etcd.svg" alt="stacked etcd topology">
<small>
Diagram representing this arch, that I stole from the <a href="https://kubernetes.io/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg">k8s docs</a>
on this topic.
</small>
<hr>

<h3 id='upgrading_cluster'><a href='#upgrading_cluster'>Upgrading Cluster Version</a></h3>
<p>
It is recommended to upgrade from a minor version to a next higher one, say,
<code>1.18.0</code> to <code>1.19.0</code>,  or from a patch version to a
higher one, <code>1.18.0</code> to <code>1.18.3</code>
</p>

<p>
The high level plan is this:
<ol>
    <li>
        <p>Upgrade a primary control plane node</p>
    </li>
    <li>
        <p>In case of HA, upgrade additional control planes</p>
    </li>
    <li>
        <p>Upgrade worker nodes</p>
    </li>
</ol>
</p>

<blockquote>
<p>
One last thing before going to the steps. You are going to see that when we
<code>drain</code> a node we use the <code>--ignore-daemonsets</code> flag.
Which begs the question, what is a daemonset?
</p>
<p>
A daemonset defines pods needed for node-local stuff, say you want to have
a daemon on each node that collects logs. You can deploy a daemonset for it.
When we drain a node to upgrade we tell it to not kick out of there the
daemonsets, since we might actually need those for the node to operate
properly.
</p>
</blockquote>

<h4 id='upgrade_control_planes'><a href='#upgrade_control_planes'>Upgrade Control Planes</a></h4>
<ol>
    <li>
        <p><code>ssh</code> into the node</p>
    </li>
    <li>
        <p><code>k get nodes</code> to check current version</p>
    </li>
    <li>
        <p>
            Use your package manager <code>apt</code>/<code>dnf</code> and
            upgrade <code>kubeadm</code>
        </p>
    </li>
    <li>
        <p>
            Check which <code>kubeadm</code> versions are available to upgrade
            to
            <pre>
            <code>
$ sudo kubeadm upgrade plan
...
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.18.20
[upgrade/versions] kubeadm version: v1.19.0
I0708 17:32:53.037895   17430 version.go:252] remote version is much newer: \
v1.21.2; falling back to: stable-1.19
[upgrade/versions] Latest stable version: v1.19.12
[upgrade/versions] Latest version in the v1.18 series: v1.18.20
...
You can now apply the upgrade by executing the following command:

    kubeadm upgrade apply v1.19.12

Note: Before you can perform this upgrade, you have to update kubeadm to v1.19.12.
...
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p>Upgrade it <code>kubeadm upgrade apply v1.19.12</code></p>
    </li>
    <li>
        <p>
            Then we need to <u>drain</u> the node. Which means we mark the node
            as unschedulable, and new pods wont arrive.
            <pre>
            <code>
kubectl drain kube-control-plane --ignore-daemonsets
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p>
            Use your package manager to upgrade both <code>kubelet</code> and
            <code>kubectl</code> to the same version
        </p>
    </li>
    <li>
        <p>
            Restart and reload <code>kubelet</code> daemon with
            <code>systemctl</code>
        </p>
    </li>
    <li>
        <p>
            Mark node as schedulable again
            <pre>
            <code>
k uncordon kube-control-plane
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p><code>k get nodes</code> should show the new version</p>
    </li>
</ol>

<h4 id='upgrade_workers'><a href='#upgrade_workers'>Upgrade Workers</a></h4>

<ol>
    <li>
        <p><code>ssh</code> into the node</p>
    </li>
    <li>
        <p><code>k get nodes</code> to check current version</p>
    </li>
    <li>
        <p>
            Use your package manager <code>apt</code>/<code>dnf</code> and
            upgrade <code>kubeadm</code>
        </p>
    </li>
    <li>
        <p>
            Do <code>kubeadm upgrade node</code> to upgrade the
            <code>kubelet</code> configuration
        </p>
    </li>
    <li>
        <p>
            Drain the node as we did with the control plane
            <pre>
            <code>
kubectl drain worker-node --ignore-daemonsets
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p>
            Use your package manager to upgrade both <code>kubelet</code> and
            <code>kubectl</code> to the same version
        </p>
    </li>
    <li>
        <p>
            Restart and reload <code>kubelet</code> daemon with
            <code>systemctl</code>
        </p>
    </li>
    <li>
        <p>
            Mark node as schedulable again
            <pre>
            <code>
k uncordon worker-node
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p><code>k get nodes</code> should show the new version</p>
    </li>
</ol>
</section>

<h2 id='etcd'><a href='#etcd'>etcd</a></h2>
<section>
<p>
etcd is a key-value store used as k8s backing store for all the cluster
information. They are a stand alone project with its own <a
href="https://etcd.io/">docs</a>. Since it is used for backup, we need to know
how to use it in order to restore or backup the cluster.
</p>

<p>
There are two cli's we will be working with <code>etcdcutl</code> and
<code>etcdutl</code>.
<ul>
    <li>
        <p>
            <code>etcdctl</code>: primary way to interact with etcd over the
            network.
        </p>
    </li>
    <li>
        <p>
            <code>etcdutl</code>: designed to operate with etcd data files
            directly, not over the network.
        </p>
    </li>
</ul>
</p>

<p>
<code>kubeadm</code> will setup etcd as pods managed directly by the kubelet
daemon (known as <i>static pods</i>). You can actually see them by runnin g
</p>

<h3 id='backup_etcd'><a href='#backup_etcd'>Backing up etcd cluster</a></h3>
<p>
All k8s data is stored in etcd, this includes sensitive data, therefore the
snapshots created by etcd are encrypted.
</p>

<p>
In order to talk to etcd we can <code>ssh</code> into the control plane, then
do <code>etcdctl version</code> to verify it is installed.
</p>

<p>
If you went with <code>kubeadm</code> as your installation way, you can see
that there is a pod in the <code>kube-system</code> namespace that concerns
etcd. If you <code>describe</code> it you will some information relevant to
connect to etcd.
<pre>
<code>
k describe pod etcd-cka-control-plane -n kube-system | grep '\-\-'
      --listen-client-urls=https://10.2.0.9:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
</code>
</pre>

</p>
If  we want to talk to etcd from outside the control plane node, we will need
the <code>--listen-client-urls</code> addresses. If you are inside the node,
you can skip that. We are going to need the path to all the TLS things. A
simple command you can test if you have everything right is the following
<pre>
<code>
ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
    member list
    bbf4baa696b33a2e, started, control-plane, https://10.2.0.9:2380, https://10.2.0.9:2379
</code>
</pre>
Since the certificates are inside a path which your user probably does not have
access, you will have to <code>sudo</code> it.
</p>

<p>
The you can create an snapshot by running the <code>snapshot save
/path/to/new/snapshot</code> command.
<pre>
<code>
ETCDCTL_API=3 etcdctl --endpoints https://162.243.29.89:2379 \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt  \
    snapshot save snapshot.db
2025-08-09 17:04:29.201951 I | clientv3: opened snapshot stream; downloading
2025-08-09 17:04:29.241278 I | clientv3: completed snapshot read; closing
Snapshot saved at snapshot.db
</code>
</pre>
</p>

<h3 id='restore_etcd'><a href='#restore_etcd'>Restore from snapshot</a></h3>

<p>
We will use the <code>etcdutl</code> to restore a snapshot.
<pre>
<code>
etcdutl --data-dir /path/to/be/restored/to snapshot restore snapshot.db
</code>
</pre>
</p>

<p>
We also need to point the etcd pod to this new path we have restored the info
to. You can find the manifest for the etcd pod under
<code>/etc/kubernetes/manifests/etcd.yaml</code>. There is a volume called
<code>etcd-data</code>, point it to the new path, and restart the pod
</p>
</section>

<h2 id='auth'><a href='#auth'>Control Access to the k8s API</a></h2>
<section>
<p>
The way anyone talks with k8s, is through the API, it does not matter if you
are a human, or a service account you all talk to the k8s http api. When a
request gets to the server it goes through some stages, shown in the <a
href="https://kubernetes.io/docs/concepts/security/controlling-access/">docs</a>
diagram which I copy pasted here:
<img src="/static/img/cka/access-control-overview.svg" alt="access-control-overview">
</p>
<h3 id='api_tls'><a href="#api_tls">Transport Security</a></h3>
<p>
All the requests go through TLS, by default the API will run on
<code>0.0.0.0:6443</code> but this can be changed , with
<code>--secure-port</code> and the <code>--bind-address</code> flags.
</p>

<p>
When you <code>kubeadm init</code> your cluster, k8s will create its own
Certificate Authority (CA) and its key (<code>/etc/kubernetes/pki/ca.crt and
/etc/kubernetes/pki/ca.key</code> respectively). It will use this to sign the
certificates used by the API server.
</p>

<p>
Inside your <code>.kube/config</code> file you will need a copy of that
certificate, this verifies that the API's certificate is authentic and was
signed with the clusters CA.
</p>

<h3 id='api_authentication'><a href="#api_authentication">Authentication</a></h3>
<p>
Once we have TLS, we can continue with authentication. The cluster admin may
setup different authentication modules, if so they will be tried sequentially
to see any suffices.
</p>
<p>
K8s may use the whole http request to authenticate, although most modules only
use the headers.
</p>
<p>
If all the modules failed, then a <code>401</code> will be returned. If it is
successful, the user is authenticated as an specific <code>username</code>.
</p>

<h3 id='api_authorization'><a href="#api_authorization">Authorization</a></h3>
<p>
Once the request has passed the authentication stage, it is time to see if it
can in fact do the action it was trying to accomplish. The request will must
include its username, a requested action, and the resource affected by the
action. The request then will be authorized if there is an existing policy that
declares that the user has permissions to do the action it is intended to.
</p>

<p>
There are different authorization modules, the administrator can setup many in
one cluster, they will be tried one by one and if all fails a <code>403</code> will be
returned
</p>


<h3 id='admission_control'><a href="#admission_control">Admission Control</a></h3>

<p>
If the authorization is successful, then we jump to admission controles. They
are basically a piece of code that will check the data arriving in a request
that modifies a resource. They do not control requests to <i>read</i>
resources, only those that modify them. They usually just validate stuff. The
thing is that if one fails the request is rejected, it is not like the others
stages where we try one by one.
</p>

<h3 id='auditing'><a href="#auditing">Auditing</a></h3>
<p>
Generate a chronological set of records, documenting everything that is
happening.
</p>

<h3 id='rbac'><a href="#rbac">Using RBAC Authorization</a></h3>
<p>
Role-based access control is a way of controlling access to network resources
based on the roles an individual has. The
<code>rbac.authorization.k8s.io</code> api group, allows you to set them up
dynamically in the k8s cluster.
</p>
<h4 id='rbac_api'><a href="#rbac_api">API objects</a></h4>
<p>
RBAC introduces 4 new object types to the cluster, <code>Role</code>,
<code>ClusterRole</code>, <code>RoleBinding</code>,
<code>ClusterRoleBinding</code>.
</p>
<p>
<ul>
  <li>
    <b> <code>Role</code> and <code>ClusterRole</code></b>
    <p>
        These represent a set of permissions. The only difference between the
        two is that <code>Role</code> defines the permissions for a namespace,
        and <code>ClusterRole</code> is not limited to a namespace.
    </p>
    <ul>
      <li>
        <b><code>Role</code></b>
          <p>
            Here is the command for creating a role to get and watch all the
            pods in the nginx namespace.
            <pre>
            <code>
k create role --dry-run=client -o yaml pod-reader --resource=pod --verb=get,watch -n nginx
            </code>
            </pre>
          </p>
          <p>
            To be honest you might be better just going to the docs and copy
            the manifest from there, since it can get a bit long to write all
            the verbs and resources in one command.
          </p>
      </li>
      <li>
        <b><code>ClusterRole</code></b>
        <p>
          Since these are not bound to a namespaces you can also use them to
          set permissions to things like nodes and persistent volumes.
        </p>
        </p>
          The command is super similar, just we do not specify a namespace
          <pre>
          <code>
k create clusterrole secret-watcher --resource=secret --verb=get,list --dry-run=client -o yaml
          </code>
          </pre>
        </p>
        <p>
          Another thing specific to <code>ClusterRole</code>s is that you can
          aggregate them. When you create one you can add a label to it. Then
          you can create another one that uses that label.
          <pre>
          <code>
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring-aggregate
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.example.com/aggregate-to-monitoring: "true"
rules: []
          </code>
          </pre>
          In this example we are adding all <code>ClusterRole</code>s that have
          the label <code>rbac.example.com/aggregate-to-monitoring: "true"
          </code>.
        </p>
      </li>
    </ul>
  </li>
  <li>
    <b> <code>RoleBinding</code> and <code>ClusterRoleBinding</code></b>
    <p>
        Once you created your <code>Role</code> object you can bind it to a
        user, or service account. This makes <code>Role</code>s reusable. For
        example, you can create a <i>pod-read-only</i>, and bind it to many
        <code>subjects</code> (users, groups or service accounts).
    </p>
    <p>
        A <code>RoleBinding</code> may bind any <code>Role</code>  in the same
        namespace, buuut you can also use them to bind
        <code>ClusterRoles</code>, to a single namespace.
    </p>
    <p>
      You can create them as you would expect
      <pre>
      <code>
k create rolebinding pod-reader --dry-run=client -o yaml  --role=pod-readonly --user=jose
      </code>
      </pre>
      Remember you can always <code>--help</code> stuff, or copy an
      example from the wiki
    </p>
    <p>
      You cannot patch/edit an existing rolebinding to change the roles. You
      have to delete it and create one again. There is this <code>kubectl auth
      reconcile</code> which will do that for you.
    </p>
  </li>
</ul>
</p>
<p>
One last tip, you can always do
<pre>
<code>
 k auth can-i get pod/logs --as="some-subject" -n "ns" # can-i verb resource
</code>
</pre>
To check if the role is working as expected.
<p>

<h3 id='sa'><a href="#sa">Service Account</a></h3>

<p>
A service account is a non-human account that provides an identity in a k8s
cluster. Pods can use them to do requests against k8s api, to authenticate
against a image registry.
</p>

<p>
They are represented in the k8s cluster with the <code>ServiceAccount</code>
object. They are <b>namespaced</b>, <b>lightweight</b>, <b>portable</b>.
</p>

<p>
There is also this <code>default</code> service account created in every
namespace. If you try to delete it the control plane replaces it. This account
is assign to all pods if you do not manually assign one, and has api discovery
permissions.
</p>

<p>
You can use <code>RBAC</code> to add roles to it. It is just another subject
you can include in the roles manifests.
</p>

<p>
To use one you just have to:
<ul>
    <li>
        <p>Create the service account, in a declarative or imperative way</p>
    </li>
    <li>
        <p>Give it roles with RBAC</p>
    </li>
    <li>
        <p>Assign it to a pod during its creation</p>
    </li>
</ul>
If you need its identity for an external service you need to get a token.
<pre>
<code>
k create token "sa-name" -n test-token
</code>
</pre>
</p>

<p>
To assign one to a pod just add the <code>spec.serviceAccountName</code> field.
</p>
</section>

<h2 id='ops_crd'><a href='#ops_crd'>Operators and Custom Resources</a></h2>
<section>
<h3 id='cr'><a href='#cr'>Custom Resources</a></h3>
<p>
A <i>resource</i> is an endpoint in k8s api, that manages objects of the same type.
One example is the <code>pods</code> resource; it is an endpoint of the api and
you use it to create, destroy, list pod objects.
</p>

<p>
Then a <i>custom resource</i> is an extension of k8s native api. You can
create your own resources for your own needs. Custom resources can be created
and destroyed dynamically on a running cluster, and once installed you can use
<code>kubectl</code> to manage them as you would manage any other resource.
</p>
<p>
Say, you might have one to create a <i>database</i> custom resource to
represent and manage it inside of your cluster.
</p>
<h3 id='cc'><a href='#cc'>Custom Controllers</a></h3>
<p>
A <i>custom resource</i> by itself will only represent some structured data. To
make them work in a true declarative state you need to add also a controller.
</p>
<p>
In an imperative API you tell the server to do something and it does it. In a
declarative api, like k8s is, you tell it the state you want to accomplish, in
this case using the <i>custom resources</i> endpoints, and then there will be a
controller that makes sure that state is true.
</p>
<hr>
<small>
One small disclaimer, using a custom resource is not a one size fits all
solution, you have to check if you really need to implement this as a CRD or
maybe just a separated API or even a configmap would do the trick, the <a
href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#should-i-add-a-custom-resource-to-my-kubernetes-cluster">k8s
docs</a> have a section to help you decide whether you really need them or not.

</small>
<hr>
<p>
It is outside of the scope of the exam but there are two ways of creating a
custom resource.
<ul>
    <li>
        <p>Using the <code>CustomResourceDefinition</code> api resource</p>
    </li>
    <li>
        <p>Using API server aggregation</p>
    </li>
</ul>
The first one is simple as you do not need to code anything just define the
custom resource in a manifest and the k8s api will handle storage and stuff
like that. Go to the official docs for more info.
<p>
<h3 id='ops'><a href='#ops'>Operator Pattern</a></h3>
<p>
The operator pattern is creating a custom controller to manage a custom
resource.
</p>
<p>
One example would be, <i>deploying an application on demand</i>. This would
look something like, we have a new custom resource called
<code>ApplicationDeployment</code> where the developer specifies the application
they want to deploy. Now when they <code>k apply -f</code> it, there would be a
controller that takes care of all the deployment of the app.
</p>
<p>
There are many operators already created by the community, you can find several
in the <a href="https://operatorhub.io">OperatorHub</a>. A popular one is
ArgoCD, this defines custom resources such as <code>Application</code> where
you can point to a git repository , and it will make sure the code is in sync
with that repository among other things. Popular on organizations using GitOps.
</p>
</section>

<h2 id='helm_kustomize'><a href='#helm_kustomize'>Helm and Kustomize</a></h2>
<section>
<h3 id='helm'><a href='#helm'>Helm</a></h3>
<p>
Helm is a package manager for k8s.  This means that you can use it similar
to <code>apt</code> or <code>dnf</code> to install full working packages in the
k8s cluster.
</p>
<p>
Usually a deployment of  a full service in a k8s cluster would involve multiple
resources, <code>services</code> <code>pods</code> <code>configmaps</code>. It
would be a bit complicated to deploy all of them using <code>kubectl</code>.
With helm you can deploy full working solutions with just a few commands.
</p>
<p>
Say you want to deploy jenkins in your cluster. You could just look in the <a
href="https://artifacthub.io">ArtifactHub</a> for jenkins,  and follow the
instructions for installing the chart. It typically looks something like the
following.
<ul>
    <li>
        <p>
          We first need to add the repo for helm to keep track of it.
          <pre><code>
helm repo add jenkins https://charts.jenkins.io
helm repo update
          </code></pre>
        </p>
    </li>
    <li>
        <p>
        Then you can just install it specifying a name for the release. Do not
        forget that you are using your <code>kubeconfig</code> configuration so
        the namespace and cluster you are pointing to will be the target of
        this operation.
        <pre><code>
helm install my-jenkins jenkins/jenkins --version 5.8.25 # helm install [RELEASE_NAME] jenkins/jenkins [flags]
        </code></pre>
        </p>
    </li>
    <li>
        <p>
        It will create all the k8s resources needed for it to work. The
        cool thing about this is that you can customize it a bit by passing
        values to certain variables for the package. Say you want to change
        the admin user, it varies on the package of course but here you can
        do something like:
        <pre><code>
helm install my-jenkins jenkinsci/jenkins --version 4.6.4 \
    --set controller.adminUser=boss --set controller.adminPassword=password \
    -n jenkins --create-namespace
        </code></pre>
        </p>
    </li>
    <li>
        <p>
        You can discover a list of all the values too.
            <pre><code>
helm show values jenkinsci/jenkins
            </code></pre>
        </p>
    </li>
    <li>
        You can list the installed packages
        <p>
            <pre><code>
helm list
            </code></pre>
        </p>
    </li>
    <li>
        There is also a simple way to upgrade a release
        <p>
            <pre><code>
helm repo update; # so we have the most up-to-date version
helm upgrade my-jenkins jenkinsci/jenkins --version 5.8.26
            </code></pre>
        </p>
    </li>
    <li>
        Finally you can remove them by just doing the uninstall subcommand
        <p>
            <pre><code>
helm uninstall my-jenkins
            </code></pre>
        </p>
    </li>
</ul>
</p>
<h3 id='kustomize'><a href='#kustomize'>Kustomize</a></h3>
<p>
Kustomize allow you to manage multiple k8s manifests in an easy way. It has
different capabilities.
<ul>
    <li>
        <p>
            You can build <code>configmaps</code> and other resources out of
            files.
        </p>
    </li>
    <li>
        <p>
            You can patch different values, say the DNS for an application
            based on different overlays/environments.
        </p>
    </li>
</ul>
It is really not worth going that much into detail since this will likely not
come into the certification.
</p>
<p>
Just a few quick things, the heart of this is the
<code>kustomization.yaml</code> file, there you will list all the resources
kustomize will use to render the templates.
</p>
<p>
You can also render how the manifests would look without having to apply them
with
<pre><code>
kustomize bulid /path/to/kustomization.yaml # or
k kustomize /path/to/kustomization.yaml
</code></pre>
</p>
<p>
Here is a short example on how you can start using this, say to add the same
namespace to two different manfiests.
<pre><code>
% tail -n +1  kustomization.yaml pod.yaml configmap.yaml
== kustomization.yaml ==
namespace: kustom
resources:
- pod.yaml
- configmap.yaml

== pod.yaml ==
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx:1.21.1
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

== configmap.yaml ==
apiVersion: v1
data:
  dir: /etc/logs/traffic-log.txt #/etc/logs/traffic.log
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: logs-config
</code></pre>
</p>
</section>
<hr>
<h2 id='workloads'><a href='#workloadse'>Workloads</a></h2>
<section>
<p>
A workload is an application running inside a k8s cluster. Wether your
application has different components running or just one, you will run it
inside a set of Pods. A Pod is nothing more than a set of containers.
</p>

<p>
Pods have a defined life-cycle, meaning if you kill one or it dies do to some
issue, it is not going to respawn by itself or anything.
</p>

<p>
To make life easier, k8s has a set of different controllers that will help
manage this pods. Say, always keeping 3 of them alive, even if one is killed,
spin up another one to take its place. You can use <i>workload resources</i> to
make this happen. The workload resources will configure these controllers
depending on what you want to do. We will go more in depth on each, but here is
a brief intro into each of them.
<p>
<ul>
    <li>
        <p>
            <b>Deployment</b> and <b>ReplicaSet</b>, these are good more
            managing workloads where pods are replaceable/interchangeable,
            <i>stateless</i> applications.
        </p>
    </li>
    <li>
        <p>
           <b>StatefulSet</b>, this will help you run applications where pods
           do keep track of the state. Useful when mounting Persistent Volumes
           to different pods, so they stay consistent.
        </p>
    </li>
    <li>
        <p>
          <b>DaemonSet</b>, pods that provide some functionality to Nodes,
          maybe for networking, or to manage the node. These are like daemons
          that will be assigned to each Node.
        </p>
    </li>
    <li>
        <p>
          <b>Job</b> and <b>CronJob</b>, define tasks that will run until
          completion and then stop.
        </p>
    </li>
</ul>

<h3 id='pods'><a href='#pods'>Pods</a></h3>
<hr>
<p>
A pod is like a set of containers with shared namespaces and shared file
systems. You can run just one or multiple containers in one Pod.
</p>
<h3 id='pods_life'><a href='#pods_life'>A Pod's Lifecycle</a></h3>
<p>
Pods are consider ephemeral, pods are created, assigned a unique id (UID),
scheduled to run to nodes where they will live until their termination. If a
node dies, the pods that lived there, or were scheduled to live there, will be
marked for deletion.
</p>
<p>
While a pod is running, <code>kubelet</code> can restart its containers to
handle some kind of faults.
</p>
<p>
Pods are only scheduled one in their lifetime; assigning a pod to a node is
called <i>binding</i>, and the process of selecting which node the pod should
go to is known as <i>scheduling</i>. Once a pod is scheduled to a node they are
bound until either of them dies.
</p>
<p>
A pod is never "re-scheduled", it is simply killed and replaced by maybe a
super similar one but the UID will be different.
</p>
<p>
<h4 id='pods_phases'><a href='#pods_phases'>Pod phases</a></h4>
<p>
There are several pods phases:
</p>
<table>
    <thead>
    <tr>
        <th>Phase</th>
        <th>Description</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <td>Pending</td>
        <td>
            The Pod has been accepted by k8s, but one or more containers are
            not ready to run. This means it might be waiting for scheduling or
            downloading an image from a registry.
        </td>
    </tr>
    <tr>
        <td>Running</td>
        <td>
            The Pod has been bound to a Node, all the containers have been
            created. At least one of them is running, or in the process of
            starting/restarting.
        </td>
    </tr>
    <tr>
        <td>Succeeded</td>
        <td>
            All containers in the Pod have been terminated in success.
        </td>
    </tr>
    <tr>
        <td>Failed</td>
        <td>
            All containers in the Pod have been terminated, but one ore more
            terminated in failure.
        </td>
    </tr>
    <tr>
        <td>Unknown</td>
        <td>
            We could not get the state of the pod, usually an error with
            communicating with the Node the pod is running on.
        </td>
    </tr>
    </tbody>
</table>
<blockquote>
<code>CrashLoopBackOff</code> and <code>Terminating</code> are not actually
<b>phases</b> of a pod. Make sure to not confuse status with phase.
</blockquote>
<h4 id='pods_handle_issues'><a href='#pods_handle_issues'>Pods handling issues</a></h4>
<p>
Similar to every living thing on this green Earth, a Pod will be presented with
issues along its time in this world filled with thorns and thistles. Maybe, as
us, even its own life will depend on how well it is able to solve them. This
unnecessary biblical de-tour begs the question, how does it handle problems
with containers?
</p>
<p>
The pods <code>spec</code> has a <code>restartPolicy</code>. This will
determine how k8s reacts to containers exiting due to errors.
<ol>
    <li>
        <p>
            <b>Initial Crash</b>, k8s immediately tries to restart it based on
            the <code>restartPolicy</code>
        </p>
    </li>
    <li>
        <p>
            <b>Repeated Crashes</b>, if it keeps failing, it will add an
            exponential backoff delay for the next restarts
        </p>
    </li>
    <li>
        <p>
            <b>CrashLoopBackOff state</b>, this indicates the backoff delay
            mechanism is in effect.

        </p>
    </li>
    <li>
        <p>
            <b>Backoff reset</b>, if a container manages to stay alive for a
            certain duration of time, the backoff delay is restarted.
        </p>
    </li>
</ol>
</p>
<p>
Troubleshooting is its own separated section, but here are some reasons a Pod
might be <code>CrashLoopBackOff </code>ing.
<ul>
    <li>
        <p>Application errors are causing the container to exit</p>
    </li>
    <li>
        <p>Configuration errors, missing files, or env vars</p>
    </li>
    <li>
        <p>
            Resources, the container may not have enough memory or cpu to start
        </p>
    </li>
    <li>
        <p>
            Healthchecks are failing if the application doesn't start serving
            in time.
        </p>
    </li>
</ul>
</p>
<p>
How to debug this? Check the <code>logs</code>, <code>events</code>, ensure the
configuration is set up properly, check resources limits, debug application.
Maybe even run the image locally, see if it is working fine.
</p>
<p>
A <code>restartPolicy </code> can be <code>Never</code>, <code>Always</code>,
<code>OnFailure</code>.
</p>

<h4 id='container_probes'><a href='#container_probes'>Container Probes</a></h4>
<p>
A probe is a diagnostic periodically performed by the kubelet. There are three
types the <code>livenessProbe</code>, <code>readinessProbe</code>,
<code>startupProbe</code>.
</p>

<p>
Pretty self explanatory, maybe the only thing to clarify is that the
<code>startupProbe</code> indicates if the app inside a container started. All
the other probes will be disabled until this is done. Usually this one is used
for containers that take a long time to start.
</p>

<p>
And the <code>readinessProbe</code> indicates whether the container is ready to
respond to requests.
</p>

<p>
There are 4 check mechanisms.
<ol>
    <li>
        <p>
            <code>exec</code>: exec a command inside the container, if
            successful return <code>0</code>.
        </p>
    </li>
    <li>
        <p><code>grpc</code>: performs remote call using gRPC.</p>
    </li>
    <li>
        <p>
            <code>httpGet</code>: makes a http GET request against the pod ip
            to a given endpoint.
        </p>
    </li>
    <li>
        <p>
            <code>tcpSocket</code>: perform  a tcp check, considers successful
            if the port is open
        </p>
    </li>
</ol>
</p>

<h3 id='containers'><a href='#containers'>Containers</a></h3>

<h3 id='init_container'><a href='#init_container'>Init Containers</a></h3>
<p>
An init container is one (or and array) of containers that will run before your
main application containers. They will run until completion, meaning they
cannot live side by side with your main containers. Those are sidecars, which
we will talk about later.
</p>
<p>
They run sequentially, and if one fails <code>kubelet</code> will restart that
init container until it succeed, if the <code>restartPolicy</code> is set to
never. Then when it fails the whole pod will be treated as failed.
</p>
<p>
They have all the fields and features of regular containers, they just do not
have probes.
</p>
<p>
They are useful to setup different stuff in your application. Like set up
things in volumes and stuff like that. Maybe download a file or something.
</p>
<p>
Here is an example form the docs where the init container waits for a svc in
k8s to be up and running before starting these pods containers.
<pre>
<code>
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app.kubernetes.io/name: MyApp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
</code>
</pre>
</p>

<h3 id='sidecar_container'><a href='#sidecar_container'>Sidecar Containers</a></h3>
<p>
Sidecar Containers are containers that run side by side with the application
container, they can be used for logging, data sync, monitoring, and so on.
Typically you only have one <i>application container</i> per pod. For example,
if you have a web app that requires a web server, you would have you web app in
the app container and use a sidecar container as the web server.
</p>

<p>
The implementation is super simple, just add a <code>restartPolicy</code> to an
init container and there you have it. The cool thing is that they will still be
run sequentially, but the one with the <code>restartPolicy</code> will keep
running.
</p>
<p>
You can also use probes on these containers opposed to regular init containers.
</p>
<p>
When a pod dies, first the app container will be deleted, then the sidecar
containers on opposed order to what they were spawned on.
</p>
<p>
You can have multiple containers as the main containers of the pod, so when to
use this. The app containers are meant for executing primary application logic.
That is why usually you just have one and use sidecars for anything else.
</p>
<h3 id=''><a href='#workload_management'>Workload Management</a></h3>
<hr>
<p>
Your applications will run inside pods, the k8s api offers different resources
to help you manage them. Say a pod dies, you would not like to have to go
in the middle of the night and start it again. We can use k8s objects that
will help us manage them.
</p>
<h3 id='replicaset'><a href='#replicaset'>ReplicaSet</a></h3>
<p>
A replicaset's purpose is to keep a stable set of replica pods running at any
given time. You tell it how many and how does the pod look and it will
remove/create pods to maintain this state.
</p>
<p>
In a replicaset's fields you specify a selector, which tells the replicaset how
to identify pods it can acquire and a number specifying how many pods it should
be  maintaining.
</p>
<p>
You are not likely to create a replicaset by itself, you usually create higher
level resources like a Deployment that then will use ReplicaSets.
</p>
<h3 id='deploy'><a href='#deploy'>Deployments</a></h3>
<p>
A deployment manages a set of pods that run an application. You describe the
desired state for the application, say, <i>how many pods need to be
available?</i> and the controller will maintain that state.
</p>
<p>
There are some fields worth mentioning in a deployments' manifest. First, the
<code>spec.replicas</code> field will specify the number of replicas, of
course. We also have <code>spec.selector</code> which tells the replicaset how
to find the pods to manage, usually this one will match the label set in the
<code>spec.template</code> pod template.
</p>
<p>
Once you <code>k apply</code> deployment, there are some commands that
will come in handy.
<ul>
    <li>
        <p>
            <code>k get deploy</code> will give you overall information on the
            deployments; how many replicasets have been created, how many are
            available, the age.
        </p>
    </li>
    <li>
        <p>
            <code>k rollout status deployment/some-deployment</code> will print
            a message telling you how many replicas have been rollout, and
            similar stuff.
        </p>
    </li>
    <li>
        <p>
            <code>k get rs</code> will print the status for the replicasets
        </p>
    </li>
</ul>
<p>
If you update an image from the deployment <code>k set image
deployment/some-deploy nginx=nginx:latest</code> and do <code>k get rs</code>
you will see that there are two now. The one with the previous image that now
marks its pods as 0 and the new one with the pods marked as the
<code>spec.replicas</code> says.
</p>
<p>
The deployment controller ensures that only an specific number of pods are down
wile being updated. By default it makes sure that at least <code>3/4</code> of
the desired number of pods are up. Meaning only <code>1/4</code> can be
unavailable.
</p>
<p>
When updating, the controller will look for existing replicasets that control
certain <code>spec.label</code> but do not match the existing
<code>spec.template</code>, and scale those down. While a new replicaset with
the new <code>spec.template</code> is scaled up.
</p>
<h4 id='deploy_rollback'><a href='#deploy_rollback'>Rollback</a></h4>
<p>
If you update a deployment but it is not going the way you wanted, you can
easily go back to the previous version of your deployment. First you need to
check the rollout history, to choose what version will you rollback to.
<pre><code>
kubectl rollout history deployment/nginx-deployment
</code></pre>
You can see more details on a rollout by using the same command but with the
<code>--revision=n</code> command.
</p>
<p>
If you decide to rollback you can do
<pre>
<code>
kubectl rollout undo deployment/nginx-deployment --to-revision=n
</code>
</pre>
</p>

<h4 id='deploy_scale'><a href='#deploy_scale'>Scale</a></h4>
<p>
You can scale the replicas in a deployment with
<pre><code>
kubectl scale deployment/nginx-deployment --replicas=10
</code></pre>
If Horizontal Pod Autoscaler is setup you can setup it up based on cpu/memory
usage.
<pre><code>
kubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80
</code></pre>
</p>

<h4 id='deploy_strategy'><a href='#deploy_strategy'>Strategy</a></h4>
<p>
The <code>spec.strategy</code> field, will tell you the type of strategy used
to replace old pods by new pods. I can either be, <i>Recreate</i> or
<i>RollingUpdate</i>. The latter is the default value.
</p>
<p>
If <i>Recreate</i>, all pods are killed before new ones are created.
</p>
<p>
If <i>RollingUpdate</i>, one replicaset is scaled down while a new one is
scaled up. You can specify <code>maxUnavailable</code> and
<code>maxSurge</code> to control this.
<ul>
    <li>
        <p>
            <b>Max Unavailable</b>: tells how many pods can be unavailable
            during the updating process. If set 30%, the deployment will scale
            down the old replicaset to 70% of its capacity, and will not scale
            it further down until the new pods in the new replicaset are ready.
            Making sure that always at least 70% of the pods are available.
        </p>
    </li>
    <li>
        <p>
            <b>Max Surge</b>: this specifies how much the number of pods can go
            over the limit specified in the replicaset. Say you have 10 pods
            running and set this to 3; when the upadate starts the controller
            will scale the total number of pods to 13. If this number is
            higher, the update will be faster, but at the expense of using more
            resources.
        </p>
    </li>
</ul>
</p>

<h3 id='sts'><a href='#sts'>StatefulSets</a></h3>
<p>
There are similar to deployments, but they maintain a sticky identity to each
of the pods they create.
</p>
<p>
You will use stateful sets if you need:
<ul>
    <li>
        <p>
            If you need stable network identities, meaning, your pods have the
            same name after (re)scheduling, opposed to a random hash at the end
            of their name.
        </p>
    </li>
    <li>
        <p>
            You want to specify a PVC per pod, and do not have them fight for
            one pre-defined.
        </p>
    </li>
</ul>
</p>

<h3 id='resource_management'><a href='#resource_management'>Compute Resource Management</a></h3>
<p>
In a pod you can specify how much of a resource (RAM an CPU) a container needs.
You can set a <i>request</i> of certain amount of resources, and the scheduler
uses this information to know on which node to put it. You can also set a
<i>limit</i> on how many resources a container can use, and
<code>kubelet</code> will make sure the running container does not exceeds
those.
</p>
<p>
A pod may use more resources than it <i>requested</i>, as long as the node has
enough of them there will be no issue.
</p>
<p>
<i>Limits</i> work differently though. They are enforced by the linux kernel.
For <code>cpu</code> they are hard limits, the kernel will restrict access to
the CPU based on its limit by CPU throttling. For <code>memory</code> the
kernel uses out of memory (OOM) kills. This does not mean that as soon as the
container exceeds the memory it is killed, the kernel will only kill it if it
detects memory pressure.
</p>
<p>
You specify <code>cpu</code> and <code>memory</code> limits/resources using
specific units. Kuberenetes CPU units, and bytes respectively.
</p>
<p>
Usually you specify the limits at container level
<pre><code>
spec.containers[].resources.limits.cpu
spec.containers[].resources.limits.memory
spec.containers[].resources.requests.cpu
spec.containers[].resources.requests.memory
</code></pre>
But since <code>v1.32</code> you can also set them a pod level
<pre><code>
spec.resources.limits.cpu
spec.resources.limits.memory
spec.resources.requests.cpu
spec.resources.requests.memory
</code></pre>
</p>

<h3 id='resource_quotas'><a href='#resource_quotas'>Quotas</a></h3>
<p>
You can limit the resources by namespace using a k8s resource called
<code>ResourceQuota</code>. These are not limited to only <code>memory</code>
and <code>cpu</code>, since you can limit the amount of objects that can be
created in a namespace, say only create 10 pods or something like that.
</p>
<p>
Users need to specify the resource limit or request on their workloads if not
the API may not give permission to create them. This can be a bit painful for
developers, so you can define <code>LimitRange</code> to set defaults on pods
that do not specifically set the requirements.
</p>
<p>
Important note on all these, they do not apply to running pods, they only apply
to new pods. So if you have some deployment and then set a LimitRange expecting
the pods from that deployment to apply it you are wrong.
</p>
<p>
Here is an example on how they look like:
<pre><code>
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - default: # this section defines default limits
      cpu: 500m
    defaultRequest: # this section defines default requests
      cpu: 500m
    max: # max and min define the limit range
      cpu: "1"
    min:
      cpu: 100m
    type: Container
</code></pre>
<p>
One last thing, the LimitRange wont check if your limits make sense. If you
specify a limit less than your request, it will let you fail.
</p>
</section>
<hr>
<h2 id='network'><a href='#network'>Network</a></h2>
<section>
<p>
The k8s network model has several pieces:
<ul>
    <li>
        <p>
            Each <b>pod</b> had its own unique cluster-wide ip.
        </p>
        <p>
            A pod has its own private network which is shared by all the
            containers running in the pod; they can talk to each other using
            <code>localhost</code>.
        </p>
    </li>
    <li>
        <p>
            The <b>pod network</b> handles communication between pods. It makes
            sure pods can communicate with each other regardless of the node
            they are in. This also allows for node deamons to talk to the pods
            living on the same node.
        </p>
    </li>
    <li>
        <p>
            The <b>Service</b> api, provides a long lived IP address/hostname
            for a service implemented by pods. The pods can be replaced but the
            service will stay the same.
        </p>
        <p>
           There is another object called <code>EndpointSlice</code> which
           provides information about the pods currently working for a service.
        </p>
    </li>
    <li>
        <p>
            The <b>Gateway</b> API, (or its predecessor, Ingress), allows you
            to make a <code>svc</code> accessible to clients outside the
            cluster.
        </p>
    </li>
    <li>
        <p><b>NetworkPolicy</b> allows you to control traffic between pods</p>
    </li>
</ul>
</p>
<h3 id='policies'><a href='#policies'>Network Policies</a></h3>
<p>
You can specify how a pod is allowed to communicate to different entities over
the network using Network Policies. They are dependant on the network plugin
you used, but you usually can specify which namespaces, which pods, or which IP
blocks are allowed to send and receive traffic to/from a pod.
<p>
<p>
you go for the namespaces/pods NetworkPolicy you will use a selector to tell
what traffic is allowed.
</p>
<p>
If you go with the IP blocks you will define CIDR ranges.
</p>
<p>
Two things worth mentioning, the pod will always allow traffic between the node
and itself, and a pod cannot block access to itself.
</p>
<h4 id='pod_iso'><a href='#pod_iso'>Pod Isolation</a></h4>
<p>
There are two types of pod isolation, <code>egress</code> and
<code>ingress</code>. They are declared independently.
</p>
<p>
<ul>
    <li>
        <p>
            <code>egress</code> will tell us who the pod is allowed to send
            traffic to; meaning who it can speak to.
        </p>
    </li>
    <li>
        <p>
            <code>ingress</code> will tell us who the pod is allowed to receive
            traffic from; meaning who it can listen from.
        </p>
    </li>
</ul>
</p>
<p>
By default a pod will allow all outbound (egress) and inbound (ingress)
connections. You can create NetworkPolicy resources where the selector matches
a pod and applies its rules to, they are accumulative.
</p>

<h4 id='netpol_resource'><a href='#netpol_resource'>NetworkPolicy</a></h4>
<p>
Here is an example from the docs
<pre><code>
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
</code></pre>
</p>
<p>
A few fields worth mentioning:
<ul>
    <li>
        <p>
            <code>spec.podSelector</code>: selects the group of pods to which
            the policy will apply. An empty one will select all pods in the
            namespace.
        </p>
    </li>
    <li>
        <p>
            <code>spec.policyTypes</code>: this may include Ingress, Egress, or
            both.
        </p>
    </li>
    <li>
        <p>
            <code>spec.egress</code>: list of allowed egress rules. Has a
            <code>to</code> and <code>ports</code> sections.
        </p>
    </li>
    <li>
        <p>
            <code>spec.ingress</code>: list of allowed ingress rules. Has a
            <code>from</code> and <code>ports</code> sections.
        </p>
    </li>
</ul>
</p>
<p>
Be careful, these two configs are different.
<pre><code>
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        user: alice
    podSelector:
      matchLabels:
        role: client
</code></pre>
Here are accepting traffic from pods that are in the namespace with the label
<code>user: alice</code> <b>and also</b> they need to have the label
<code>role: client</code>.
</p>
<p>
<pre><code>
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        user: alice
  - podSelector:
      matchLabels:
        role: client
</code></pre>
Here you are accepting traffic from pods that are in the namespace with that
label <b>or</b>  from pods that have that label.
</p>
<p>
For the <code>ipBlock</code> the IP blocks you select must be cluster-external
IPs since Pod IPs are ephemeral.
</p>
<p>
One last thing worth mentioning, to target a namespace by name you will have to
use the immutable label <code>kubernetes.io/metadata.name</code>.
</p>
<h3 id='svcs'><a href='#svcs'>Services</a></h3>
<p>
If you do a deployment to your cluster that serves as a backend for an
application you want to access over the network. It can get tricky, because the
pods of a replicaset are ephemeral, their IPs will change all the time. Your
front end is not expected to update the address every time something happens.
</p>
<p>
This is why we have services, these allow you to select a group of pods using
label selectors (so if one is killed and spawned it will still be picked up)
and assign an IP that wont change in your cluster.
</p>
<p>
This way, pods can be killed and respawned but the frontend only has to keep
track of 1 address.
</p>
<p>
In the service definition you will specify the selector and ports you want to
use target.
<pre><code>
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app.kubernetes.io/name: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
</code></pre>
</p>
<p>
The pods selected by that label will form a resource called
<code>EndpointSlices</code> which basically will do the mapping, the controller
for that service will update the virtual IPs if they change.
</p>
<p>
You can define names for ports inside pods, which then you can use as a
reference in the service.
</p>
<p>
You can also create the services for a deployment  with the <code>k
expose</code> command.
</p>
<h4 id='svc_types'><a href='#svc_types'>Service Types</a></h4>
<p>
<ul>
    <li>
        <p>
            <code>ClusterIP</code> makes the service reachable from within the
            cluster.
        </p>
    </li>
    <li>
        <p>
            <code>NodePort</code>, map the service to a port on the node, this
            will give it outside access.
        </p>
    </li>
    <li>
        <p>
            <code>LoadBalancer</code> exposes the service externally using a
            load balancer. k8s does not offer a load balancing component you
            will have to use a cloud provider or something.
        </p>
    </li>
    <li>
        <p>
            <code>ExternalName</code>, map the serivce to the externalName
            field, say <code>api.foo.example</code>, this setup the cluster's
            DNS server to return a CNAME record with that hostname value.
        </p>
    </li>
</ul>
</p>
<h3 id='gateway_api'><a href='#gateway_api'>Gateway API</a></h3>
<p>
The Gateway API are some k8s resources that provide traffic routing, and make
network services available. They are role-oriented, meaning each level of
resource is supposedly manage by different personas, infra engineer, cluster
admin, and developers. Here is a list of the 3 levels.
<ul>
    <li>
        <p>
            <code>GatewayClass</code>: these are managed by the infra engineer,
            they are similar to a <code>StorageClass</code> as in they are not
            limited to namespaces, they are cluster-scoped, and usually given
            by the cloud provider. This is how the cloud provider handles
            requests from the outside world.
        </p>
        <p>
            They are as simple as this:
            <pre><code>
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: example-class
  spec:
    controllerName: example.com/gateway-controller
            </code></pre>
        </p>
    </li>
    <li>
        <p>
            <code>Gateway</code>: these  describe how traffic can be translated
            to Services within the cluster.
<pre><code>
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: example-gateway
spec:
  gatewayClassName: example-class
  listeners:
  - name: http
    protocol: HTTP
    port: 80
</code></pre>
            This basically is saying, create a gateway using the class
            specified there, and listen on port <code>80</code>.
        </p>
    </li>
    <li>
        <p>
            <code>HTTPRoute</code>, tells the behaviour of http requests from
            the gateway listener.
<pre><code>
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: example-httproute
spec:
  parentRefs:
  - name: example-gateway
  hostnames:
  - "www.example.com"
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /login
    backendRefs:
    - name: example-svc
      port: 8080
</code></pre>
            Here we are telling the gateway we just created, that if in the
            <code>Host:</code> of the request you find
            <code>www.example.com</code> and that the path is
            <code>/login</code> then you should use <code>example-svc</code> on
            port 8080
        </p>
    </li>
</ul>
</p>
<h3 id='ingress'><a href='#ingress'>Ingress</a></h3>
<p>
An Ingress is an object that manges external access to a <code>svc</code>.
Here you can define a hostname, tls among other things.
</p>
<p>
To make them work in your cluster you need to first have an Ingress Class,
there are a few you can choose from like <a
href="https://kubernetes.github.io/ingress-nginx/user-guide/basic-usage/">ingress-nginx
controller</a>.
</p>
<p>
The imperative way for creating one is actually a good way to understand them
to. Look at the command:
<pre><code>
k create ing website-api --rule='website.com/api=my-svc:8080'
</code></pre>
The rule part is will tell the cluster that if a request gets, where the host
is <code>webiste.com</code> and the path is <code>/api</code> it should map it
to the <code>svc</code> called <code>my-svc</code> on port <code>8080</code>.
Basically <code>host/path=service:port</code>.
</p>
<p>
In that example we are not specifying <code>tls</code>, but you can do it by
pointing to a secret of that type.
</p>
<p>
There are some path types, like if the <code>/api</code> you specified should
be a exact match (<code>Exact</code>) or can be a prefix
(<code>Prefix</code>)and stuff like that.
</p>
<h3 id='coredns'><a href='#coredns'>CoreDNS</a></h3>
<p>
You can talk with pods and services within the cluster using its dns. It is as
simple as following this structure
<code>name.namespace.type.cluster.local</code>
</p>
<p>
In order to do this, k8s runs a DNS server implementation called CoreDNS, if
you get the pods from <code>kube-system</code> you will be able to see the pod
that is running this. The config is in a <code>cm</code> called coredns in the
same namespace.
</p>
</section>

<hr>
<h3 id='table'>~ Table of Contents</h3>
<ul>
  <li><a href='#exam_details'>Exam Details</a></li>
  <li>
    <a href='#k8s_nutshell'>K8s in a Nutshell</a>
    <ul>
      <li><a href='#features'>Features</a></li>
      <li><a href='#high_level_arch'>High-Level Architecture</a></li>
      <li><a href='#control_plane_nodes'>Control Plane Nodes</a></li>
      <li><a href='#shared_components'>Components Shared by Nodes</a></li>
      <li><a href='#advantages'>Advantages of Using k8s</a></li>
    </ul>
  </li>
  <li>
    <a href='#interacting_k8s'>Interacting with K8s</a>
    <ul>
      <li><a href='#api_primitives'>API Primitives and Objects</a></li>
      <li><a href='#kubectl'>kubectl</a></li>
      <li>
        <a href='#managing_objects'>Managing Objects</a>
        <ul>
          <li><a href='#imperative'>Imperative</a></li>
          <li><a href='#declarative'>Declarative</a></li>
          <li><a href='#hybrid'>Hybrid</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <a href='#cluster_installation'>Cluster Installation and Upgrade</a>
    <ul>
      <li><a href='#provision_infra'>Get Infrastructure Ready</a></li>
      <li><a href='#extension_interfaces'>Extension Interfaces</a></li>
      <li><a href='#setup_cluster'>Setup Cluster</a></li>
      <li><a href='#highly_available'>Highly Available (HA) Cluster</a>
        <ul>
          <li><a href="stacked_etcd">Stacked etcd topology</a></li>
          <li><a href="external_etcd">External etcd topology</a></li>
        </ul>
      </li>
      <li><a href='#upgrading_cluster'>Upgrading Cluster Version</a>
        <ul>
          <li><a href="#upgrade_control_planes">Upgrade Control Planes</a></li>
          <li><a href="#upgrade_workers">Upgrade Workers</a></li>
        </ul>
      </li>
      </li>
      <li>
        <a href='#etcd'>etcd</a>
        <ul>
          <li><a href="#backup_etcd">Backing up and etcd cluster</a></li>
          <li><a href="#restore_etcd">Restore from snapshot</a></li>
        </ul>
      </li>
      <li>
        <a href='#auth'>Control Access to the k8s API</a>
        <ul>
          <li><a href="#api_tls">Transport Security</a></li>
          <li><a href="#api_authentication">Authentication</a></li>
          <li><a href="#api_authorization">Authorization</a></li>
          <li><a href="#admission_control">Admission Control</a></li>
          <li><a href="#auditing">Auditing</a></li>
          <li><a href="#rbac">Using RBAC Authorization</a></li>
          <li><a href="#sa">Service Accounts</a></li>
        </ul>
      </li>
      <li>
        <a href='#ops_crd'>Operators and Custom Resources</a>
        <ul>
          <li><a href="#cr">Custom Resources</a></li>
          <li><a href="#cc">Custom Controllers</a></li>
          <li><a href="#ops">Operator pattern</a></li>
        </ul>
      </li>
      <li>
        <a href='#helm_kustomize'>Helm and Kustomize</a>
        <ul>
          <li><a href="#helm">Helm</a></li>
          <li><a href="#kustomize">Kustomize</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <a href='#workloads'>Workloads</a>
    <ul>
      <li>
        <a href='#pods'>Pods</a>
        <details>
          <summary>
            <a href="#pods_life">A Pod's Lifecycle</a>
          </summary>
          <ul>
            <li><a href="#pods_phases">Pods Phases</a></li>
            <li><a href="#pods_handle_issues">Pods handling issues</a></li>
            <li><a href='#container_probes'>Container Probes</a></li>
          </ul>
        </details>
        <details>
          <summary>
            <a href="#containers">Containers</a>
          </summary>
          <ul>
            <li><a href="#init_container">Init Containers</a></li>
            <li><a href="#sidecar_container">Sidecar Containers</a></li>
          </ul>
        </details>
      </li>
      <li>
        <a href='#workload_management'>Workload Management</a>
          <ul>
            <li><a href="#replicaset">ReplicaSets</a></li>
            <li><a href="#deploy">Deployments</a></li>
            <li><a href="#sts">StatefulSets</a></li>
          </ul>
      </li>
      <li>
        <a href='#resource_management'>Compute Resource Management</a>
      </li>
      <li>
        <a href='#resource_quotas'>Resource Quotas</a>
      </li>
    </ul>
  </li>
  <li>
    <a href='#network'>Network</a>
    <ul>
        <li><a href="#policies">Network Policies</a></li>
        <li><a href="#svcs">Services</a></li>
        <li><a href="#gateway_api">Gateway API</a></li>
        <li><a href="#ingress">Ingress</a></li>
        <li><a href="#corendns">CoreDNS</a></li>
    </ul>
  </li>
  </li>
  <li>
</ul>
<p><a href='#top'> go to the top</a></p>
</body>
</html>
