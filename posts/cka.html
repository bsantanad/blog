<!DOCTYPE html>
<html lang="en">
<head>
  <link rel="stylesheet" href="/static/water.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">
  <title>cka prep</title>
  <link rel="stylesheet" href="/static/highlight.min.css">
  <script src="/static/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>

    body {
      font-family: open sans, "SF Pro Display", system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue";
      max-width: 960px;
      margin: 0 auto;
    }
    h1 > a, h2 > a, h3 > a, h4 > a, h5 > a, h6 > a {
      color: black;
      text-decoration: none;
    }
    h1 > a:hover, h2 > a:hover, h3 > a:hover, h4 > a:hover, h5 > a:hover, h6 > a:hover {
      text-decoration: underline;
    }
    h1 > a:visited, h2 > a:visited, h3 > a:visited, h4 > a:visited, h5 > a:visited, h6 > a:visited {
      color: black;
    }

    a:visited {
      color: blue;
    }

    .important {
      background-color: lightgray;
      padding: 1em
    }

    .chapter {
      font-style: italic;

    }
  </style>
</head>
<body>
<a href='/'>
  < back home
</a>
<hr>
<small><i>
Crated on <time datetime="2025-07-18">July 18, 2025</time>; Last modified on
<time datetime="2025-08-15">Aug 15, 2025.</time>
</i></small>
<br>
<small>
This used to be a different page. Go to <a href="/posts/cka-old.html">cka old
notes</a> to check those out.
</small>

<h1 id='top'>~ cka prep</h1>
<a href='#table'>Table of Contents</a>

<h2 id='exam_details'><a href='#exam_details'>Sections</a></h2>
<section>
<ul>
  <li> 25% Cluster Arch, Installation and Configuration </li>
  <li> 15% Workloads and Scheduling </li>
  <li> 20% Services and Networking </li>
  <li> 10% Storage </li>
  <li> 30% Troubleshooting </li>
</ul>
</section>

<hr>
<h2 id='k8s_nutshell'><a href='#k8s_nutshell'>K8s in a Nutshell</a></h2>
<section>
<p>
K8s is a container orchestration tool. Today there are a lot of microservices
architectures. We can have a container per service and managing all those
containers can be hard. K8s takes care of the scalability, security,
persistence and load balancing.
</p>
<p>
When k8s is triggered to create a container, it will delegate it to the
container runtime engine via a CRI (container runtime interface).
</p>

<h3 id='features'><a href='#features'>Features</a></h3>
<ul>
  <li>
    <b>Declarative Model</b>
    <ul>
      <li>
        the cool thing about k8s, is that you just tell it the status
        of the cluster you want, via yamls, and it will do its best to
        create it.
      </li>
    </ul>
  </li>
  <li>
    <b>Autoscaling</b>
    <ul>
      <li>
        k8s can also automatically (or manually) scale resources when
        needed.
      </li>
    </ul>
  </li>
  <li>
    <b>Application management</b>
    <ul>
      <li>
        When deploying new versions for your application, you can use
        k8s to manage the roll out technique. As well as come back to
        previous rollouts
      </li>
    </ul>
  </li>
  <li>
    <b>Persistent Storage</b>
    <ul>
      <li>
        Containers are ephemeral, meaning the filesystem in them will
        die with them. With k8s you can have persistent volumes, to
        manage storage across containers.
      </li>
    </ul>
  </li>
  <li>
    <b>Networking</b>
    <ul>
      <li>
        k8s has internal and external load balancing for network
        traffic.
      </li>
    </ul>
  </li>
</ul>

<h3 id='high_level_arch'><a href='#high_level_arch'>High-Level Architecture</a></h3>
<p>
There are two types of nodes (these can be vm, baremetal machines, whatever
you call a computer):
</p>
<ul>
  <li>
    <b>Control plane nodes</b>
    <ul>
      <li>
        This node exposes k8s API thru a server. Whenever you do
        <code>kubectl something</code> you are talking to this API.
      </li>
    </ul>
  </li>
  <li>
    <b>Worker nodes</b>
    <ul>
      <li>
        These are the nodes that execute the workload in containers
        managed by pods. Worth noting that every worker node needs a
        container runtime engine, to create the containers.
      </li>
    </ul>
  </li>
</ul>

<h3 id='control_plane_nodes'><a href='#control_plane_nodes'>Control Plane Nodes</a></h3>
<p>These have different components to do their job:</p>
<ul>
  <li>
    <b>API Server</b>
    <ul>
      <li>
        Exposes the k8s api to clients (kubectl)
      </li>
      <li>
        Here is where the authentication, authorization and admission
        control also happens.
      </li>
    </ul>
  </li>
  <li>
    </b>Scheduler</b>
    <ul>
      <li>
        Background process that watches for new k8s pods with no
        assigned and look for a node to execute them.
      </li>
    </ul>
  </li>
  <li>
    <b>Control Manager</b>
    <ul>
      <li>
        Watches the state of your clusters and implements changes where
        needed.
      </li>
      <li>
        This is the guy that makes the cluster be to its desired state.
      </li>
    </ul>
  </li>
  <li>
    <b>Etcd</b>
    <ul>
      <li>
        key-value db that stores all k8s cluster related data.
      </li>
    </ul>
  </li>
</ul>

<h3 id='shared_components'><a href='#shared_components'>Components Shared by Nodes</a></h3>
<p>
There are some components that are shared by the nodes, whether they are
control or workers.
</p>
<ul>
  <li>
    <b>Kubelet</b>
    <ul>
      <li>
        agent that makes sure the necessary containers are running in a
        pod.
      </li>
      <li>
        this is the glue between k8s and the container runtime engine.
      </li>
    </ul>
  </li>
  <li>
    <b>Kube Proxy</b>
    <ul>
      <li>
        every node has this network proxy to enable network
        communication and implement their rules.
      </li>
    </ul>
  </li>
  <li>
    <b>Container runtime</b>
    <ul>
      <li>
        the container runtime that will manage the containers. Not
        really needed in control planes.
      </li>
    </ul>
  </li>
</ul>

<h3 id='advantages'><a href='#advantages'>Advantages on Using k8s</a></h3>
<ul>
  <li>
    <b>Portability</b>
    <ul>
      <li>
        the container runtime engine can run stuff regardless of whether
        you run it on a vm, bm or even on a raspberry. So you have
        portability for your things regardless of where the cluster is
        located.
      </li>
    </ul>
  </li>
  <li>
    <b>Resilience</b>
    <ul>
      <li>
        controllers are always looking to be on the desired state.
        Meaning it will try to self-heal when something goes array.
      </li>
    </ul>
  </li>
  <li>
    <b>Scalability</b>
    <ul>
      <li>
        k8s can scale the number of pods on demand or automatically
      </li>
    </ul>
  </li>
  <li>
    <b>Extensibility</b>
    <ul>
      <li>
        When the core functionality is not enough you can introduce CRDs
        that extend the functionality of the cluster.
      </li>
    </ul>
  </li>
</ul>
</section>

<hr>
<h2 id='interacting_k8s'><a href='#interacting_k8s'>Interacting with K8s</a></h2>
<section>

<h3 id='api_primitives'><a href='#api_primitives'>API Primitives and Objects</a></h3>
<p>
K8s has api resources which are the building blocks of the cluster. These are
your pods, deployments, services, and so on.
</p>
<p>Every k8s primitive follows a general structure:</p>
<ul>
  <li>
    <i>api version:</i>
    <ul>
      <li>
        defines the structure of a primitive and uses it to validate the
        correctness of the data.
      </li>
      <li>
        you can do <code>k api-versions</code> to see the versions
        compatible with your cluster
      </li>
    </ul>
  </li>
  <li>
    <i>kind</i>
    <ul>
      <li>
        the type of the primitive
      </li>
    </ul>
  </li>
  <li>
    <i>metadata</i>
    <ul>
      <li>
        name, namespace, high level info
      </li>
      <li>
        here you see the UID, which is an id k8s generates for each
        object.
      </li>
    </ul>
  </li>
  <li>
    <i>spec</i>
    <ul>
      <li>
        the desired state of the resource
      </li>
    </ul>
  </li>
  <li>
    <i>status</i>
    <ul>
      <li>
        actual state of the resource
      </li>
    </ul>
  </li>
</ul>

<h3 id='kubectl'><a href='#kubectl'>kubectl</a></h3>
<p>
This is how we talk to the api. Usually you do:
</p>
<pre><code>k &lt;verb&gt; &lt;resource&gt; &lt;name&gt;</code></pre>
<p>
Keep in mind we usually have different stuff in different namespaces, so we
are always appending <code>-n &lt;some namespace&gt;</code> to the command.
</p>
<p>
The name of an object has to be unique across all objects of the same resource
within a namespace.
</p>

<h3 id='managing_objects'><a href='#managing_objects'>Managing Objects</a></h3>
<p>
There are two ways to manage objects: the imperative or the declarative way.
</p>

<h4 id='imperative'><a href='#imperative'>Imperative</a></h4>
<p>
The imperative is where you use commands to make stuff happen in the cluster.
Say you want to create an nginx pod you would do:
</p>
<pre><code>k run --image=nginx:latest nginx --port=80</code></pre>
<p>
This would create the pod in the cluster when you hit enter. In my
professional experience, you hardly ever create stuff like that. The only
time I use it is to create temporary pods to test something.
</p>
<p>
There are other verbs which you might use a bit more. <code>edit</code> brings
up the raw config of the resource and you can change it on the fly. Although
I would recommend just do this for testing things. Hopefully your team has
the manifests under a version control system, if you edit stuff like this it
would mess it up.
</p>
<p>
There is also <code>patch</code> which I have never used, but it... "Update
fields of a resource using strategic merge patch, a JSON merge patch, or a
JSON patch."
</p>
<p>
There is also <code>delete</code> which -- as you probably guess already --
deletes the resource. Usually the object gets a 30 sec grace period for it to
die. But if it does not the kubelet will try to kill it forcefully.
</p>
<p>If you do:</p>
<pre><code>k delete pod nginx --now</code></pre>
<p>It will ignore the grace period.</p>

<h4 id='declarative'><a href='#declarative'>Declarative</a></h4>
<p>
This is where you have a bunch of <code>yaml</code>s which are your
definitions of resources. The cool thing about this is that you can version
control them. Say you have a <code>nginx-deploy.yaml</code>. You can create
it in the cluster with:
</p>
<pre><code>k apply -f nginx-deploy.yaml</code></pre>
<p>
This gives you more flexibility on what you are doing. Since you can just go
to the file change stuff and apply it again.
</p>

<h4 id='hybrid'><a href='#hybrid'>Hybrid</a></h4>
<p>
Usually I use a hybrid approach, most of the imperative commands have this
<code>--dry-run=client -o yaml</code> flag that you can append to the command
and it will render the yaml manifest. You can redirect that to a file and
start working on that. You open the yaml with your favourite text editor, and
then mount volumes and stuff like that.
</p>
<p>
There are more ways to manage the resources for example you can use kustomize
to render different values based on the same manifest, or with helm to bring
up complete apps/releases to just cluster. Probably we will go over them
later in the book.
</p>
</section>

<hr>
<h2 id='cluster_installation'><a href='#cluster_installation'>Cluster Installation and Upgrade</a></h2>
<section>

<h3 id='provision_infra'><a href='#provision_infra'>Get Infrastructure Ready</a></h3>

<p>
There are a million ways of doing this. I used terraform to create some
droplets in digital ocean and packer with ansible to build an image that would
let everything ready for me to run the <code>kubeadm</code> commands.
</p>

<p>
<code>kubeadm</code> is the tool to create a cluster.
</p>

<p>
Here is a non-comprehensive list of what is needed before running
<code>kubeadm</code> stuff.

<ul>
    <li>
        <p>
            Open <a
            href='https://kubernetes.io/docs/reference/networking/ports-and-protocols/'>ports
            needed</a> for k8s to work
        </p>
    </li>
    <li>
        <p>Disable swap; otherwise kubelet is going to fail to start</p>
    </li>
    <li>
        <p>Install a container runtime, like containerd</p>
    </li>
    <li>
        <p>Install <code>kubeadm</code></p>
    </li>
</ul>
</p>


<h3 id='extension_interfaces'><a href='#extension_interfaces'>Extension Interfaces</a></h3>
<p>
There are some things k8s does not have by default. You need to install this
extensions as needed.
<ul>
  <li>
    <p>
      Container Network Interface (CNI)
      <ul>
        <li>
            This manages the network interaction between containers.
        </li>
      </ul>
    </p>
  </li>
  <li>
    <p>
      Container Network Interface (CRI)
      <ul>
        <li>
            This is the piece that interacts with the Container Runtime
            (containerd, or other) to tell it what to do. Kill, create
            containers and so on.
        </li>
      </ul>
    </p>
  </li>
  <li>
    <p>
      Container Storage Interface (CSI)
      <ul>
        <li>
          This is the standard to implement plugins for interacting with
          block/file storage
        </li>
      </ul>
    </p>
  </li>
</ul>
</p>

<h3 id='setup_cluster'><a href='#setup_cluster'>Setup Cluster</a></h3>

<p>
Once you have <code>kubeadm</code> in your system everything else is pretty
straight forward. You just ssh to your control plane and run:
<pre>
<code>
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
</code>
</pre>
</p>

<p>
This runs some preflight checks to see if everything is working properly, if
not it will likely print a message telling you about what is wrong. In my case
it complained about <code>/proc/sys/net/ipv4/ip_forward</code> being disabled.
But was able to fix it by just doing <code>echo 1 | sudo tee
/proc/sys/net/ipv4/ip_forward</code>.
</p>

<p>
Where does the <code>cidr</code> comes from? I had exactly the same question.
It seems that it will depend on the CNF you will install, but do not quote me
on that.
</p>

<p>
Once the command runs successfully, it will print next steps:
<pre>
<code>
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join ip.control.panel.and:port --token some-cool-token \
	--discovery-token-ca-cert-hash sha256:some-cool-hash
</code>
</pre>
</p>

<p>
Just follow those steps. You ssh into the workers and join them with that
command. If you lost the tokens for some reason you can reprint them with:
<pre>
<code>
kubeadm token create --print-join-command
</code>
</pre>
</p>

<p>
Now, before joining the workers, you need to install the CNI, you can pick any
of the ones on <a
href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">the
k8s add-ons docs</a>.
</p>

<p>
Installing them is nothing fancy, your literally just run a `k apply -f
some-mainfest` and be done with it. I went with <a
href="https://docs.tigera.io/calico/latest/getting-started/kubernetes/flannel/install-for-flannel">calico</a>
for no particular reason.
</p>

<h3 id='highly_available'><a href='#highly_available'>Highly Available (HA) Cluster</a></h3>

<p>
The control plane is like the most important part of the cluster, since if it
fails, you are not even going to be able to talk to the API to do stuff. We can
add redundancy to improve this. Here is where HA architectures come into play.
</p>

<p>
There are two
<ul>
    <li>
        <p>Stacked etcd topology</p>
    </li>
    <li>
        <p>External etcd topology</p>
    </li>
</ul>
Remember <i>etcd</i> is key-value db k8s uses to store all of its info.
</p>

<h4 id='stacked_etcd'><a href='#stacked_etcd'>Stacked etcd topology</a></h4>
<p>
You have at least three control planes each with its own etcd in the same node.
All the nodes running at the same time and the workers talk to them through a
load balancer, if one dies, we still have others.
</p>
<img src="/static/img/cka/kubeadm-ha-topology-stacked-etcd.svg" alt="stacked etcd topology">
<small>
Diagram representing this arch, that I stole from the <a href="https://kubernetes.io/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg">k8s docs</a>
on this topic.
</small>
<hr>

<h4 id='external_etcd'><a href='#external_etcd'>External etcd topology</a></h4>
<p>
Per control plane we have two nodes, one that runs etcd and one that runs the
actual control plane stuff. They communicate through the
<code>kube-apiserver</code> api.
</p>
<p>
This topology require more nodes, and that means a bit more manage overhead.
</p>
<img src="/static/img/cka/kubeadm-ha-topology-external-etcd.svg" alt="stacked etcd topology">
<small>
Diagram representing this arch, that I stole from the <a href="https://kubernetes.io/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg">k8s docs</a>
on this topic.
</small>
<hr>

<h3 id='upgrading_cluster'><a href='#upgrading_cluster'>Upgrading Cluster Version</a></h3>
<p>
It is recommended to upgrade from a minor version to a next higher one, say,
<code>1.18.0</code> to <code>1.19.0</code>,  or from a patch version to a
higher one, <code>1.18.0</code> to <code>1.18.3</code>
</p>

<p>
The high level plan is this:
<ol>
    <li>
        <p>Upgrade a primary control plane node</p>
    </li>
    <li>
        <p>In case of HA, upgrade additional control planes</p>
    </li>
    <li>
        <p>Upgrade worker nodes</p>
    </li>
</ol>
</p>

<blockquote>
<p>
One last thing before going to the steps. You are going to see that when we
<code>drain</code> a node we use the <code>--ignore-daemonsets</code> flag.
Which begs the question, what is a daemonset?
</p>
<p>
A daemonset defines pods needed for node-local stuff, say you want to have
a daemon on each node that collects logs. You can deploy a daemonset for it.
When we drain a node to upgrade we tell it to not kick out of there the
daemonsets, since we might actually need those for the node to operate
properly.
</p>
</blockquote>

<h4 id='upgrade_control_planes'><a href='#upgrade_control_planes'>Upgrade Control Planes</a></h4>
<ol>
    <li>
        <p><code>ssh</code> into the node</p>
    </li>
    <li>
        <p><code>k get nodes</code> to check current version</p>
    </li>
    <li>
        <p>
            Use your package manager <code>apt</code>/<code>dnf</code> and
            upgrade <code>kubeadm</code>
        </p>
    </li>
    <li>
        <p>
            Check which <code>kubeadm</code> versions are available to upgrade
            to
            <pre>
            <code>
$ sudo kubeadm upgrade plan
...
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.18.20
[upgrade/versions] kubeadm version: v1.19.0
I0708 17:32:53.037895   17430 version.go:252] remote version is much newer: \
v1.21.2; falling back to: stable-1.19
[upgrade/versions] Latest stable version: v1.19.12
[upgrade/versions] Latest version in the v1.18 series: v1.18.20
...
You can now apply the upgrade by executing the following command:

	kubeadm upgrade apply v1.19.12

Note: Before you can perform this upgrade, you have to update kubeadm to v1.19.12.
...
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p>Upgrade it <code>kubeadm upgrade apply v1.19.12</code></p>
    </li>
    <li>
        <p>
            Then we need to <u>drain</u> the node. Which means we mark the node
            as unschedulable, and new pods wont arrive.
            <pre>
            <code>
kubectl drain kube-control-plane --ignore-daemonsets
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p>
            Use your package manager to upgrade both <code>kubelet</code> and
            <code>kubectl</code> to the same version
        </p>
    </li>
    <li>
        <p>
            Restart and reload <code>kubelet</code> daemon with
            <code>systemctl</code>
        </p>
    </li>
    <li>
        <p>
            Mark node as schedulable again
            <pre>
            <code>
k uncordon kube-control-plane
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p><code>k get nodes</code> should show the new version</p>
    </li>
</ol>

<h4 id='upgrade_workers'><a href='#upgrade_workers'>Upgrade Workers</a></h4>

<ol>
    <li>
        <p><code>ssh</code> into the node</p>
    </li>
    <li>
        <p><code>k get nodes</code> to check current version</p>
    </li>
    <li>
        <p>
            Use your package manager <code>apt</code>/<code>dnf</code> and
            upgrade <code>kubeadm</code>
        </p>
    </li>
    <li>
        <p>
            Do <code>kubeadm upgrade node</code> to upgrade the
            <code>kubelet</code> configuration
        </p>
    </li>
    <li>
        <p>
            Drain the node as we did with the control plane
            <pre>
            <code>
kubectl drain worker-node --ignore-daemonsets
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p>
            Use your package manager to upgrade both <code>kubelet</code> and
            <code>kubectl</code> to the same version
        </p>
    </li>
    <li>
        <p>
            Restart and reload <code>kubelet</code> daemon with
            <code>systemctl</code>
        </p>
    </li>
    <li>
        <p>
            Mark node as schedulable again
            <pre>
            <code>
k uncordon worker-node
            </code>
            </pre>
        </p>
    </li>
    <li>
        <p><code>k get nodes</code> should show the new version</p>
    </li>
</ol>
</section>

<h2 id='etcd'><a href='#etcd'>etcd</a></h2>
<section>
<p>
etcd is a key-value store used as k8s backing store for all the cluster
information. They are a stand alone project with its own <a
href="https://etcd.io/">docs</a>. Since it is used for backup, we need to know
how to use it in order to restore or backup the cluster.
</p>

<p>
There are two cli's we will be working with <code>etcdcutl</code> and
<code>etcdutl</code>.
<ul>
    <li>
        <p>
            <code>etcdctl</code>: primary way to interact with etcd over the
            network.
        </p>
    </li>
    <li>
        <p>
            <code>etcdutl</code>: designed to operate with etcd data files
            directly, not over the network.
        </p>
    </li>
</ul>
</p>

<p>
<code>kubeadm</code> will setup etcd as pods managed directly by the kubelet
daemon (known as <i>static pods</i>). You can actually see them by runnin g
</p>

<h3 id='backup_etcd'><a href='#backup_etcd'>Backing up etcd cluster</a></h3>
<p>
All k8s data is stored in etcd, this includes sensitive data, therefore the
snapshots created by etcd are encrypted.
</p>

<p>
In order to talk to etcd we can <code>ssh</code> into the control plane, then
do <code>etcdctl version</code> to verify it is installed.
</p>

<p>
If you went with <code>kubeadm</code> as your installation way, you can see
that there is a pod in the <code>kube-system</code> namespace that concerns
etcd. If you <code>describe</code> it you will some information relevant to
connect to etcd.
<pre>
<code>
k describe pod etcd-cka-control-plane -n kube-system | grep '\-\-'
      --listen-client-urls=https://10.2.0.9:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
</code>
</pre>

</p>
If  we want to talk to etcd from outside the control plane node, we will need
the <code>--listen-client-urls</code> addresses. If you are inside the node,
you can skip that. We are going to need the path to all the TLS things. A
simple command you can test if you have everything right is the following
<pre>
<code>
ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
    member list
    bbf4baa696b33a2e, started, control-plane, https://10.2.0.9:2380, https://10.2.0.9:2379
</code>
</pre>
Since the certificates are inside a path which your user probably does not have
access, you will have to <code>sudo</code> it.
</p>

<p>
The you can create an snapshot by running the <code>snapshot save
/path/to/new/snapshot</code> command.
<pre>
<code>
ETCDCTL_API=3 etcdctl --endpoints https://162.243.29.89:2379 \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt  \
    snapshot save snapshot.db
2025-08-09 17:04:29.201951 I | clientv3: opened snapshot stream; downloading
2025-08-09 17:04:29.241278 I | clientv3: completed snapshot read; closing
Snapshot saved at snapshot.db
</code>
</pre>
</p>

<h3 id='restore_etcd'><a href='#restore_etcd'>Restore from snapshot</a></h3>

<p>
We will use the <code>etcdutl</code> to restore a snapshot.
<pre>
<code>
etcdutl --data-dir /path/to/be/restored/to snapshot restore snapshot.db
</code>
</pre>
</p>

<p>
We also need to point the etcd pod to this new path we have restored the info
to. You can find the manifest for the etcd pod under
<code>/etc/kubernetes/manifests/etcd.yaml</code>. There is a volume called
<code>etcd-data</code>, point it to the new path, and restart the pod
</p>
</section>

<h2 id='auth'><a href='#auth'>Control Access to the k8s API</a></h2>
<section>
<p>
The way anyone talks with k8s, is through the API, it does not matter if you
are a human, or a service account you all talk to the k8s http api. When a
request gets to the server it goes through some stages, shown in the <a
href="https://kubernetes.io/docs/concepts/security/controlling-access/">docs</a>
diagram which I copy pasted here:
<img src="/static/img/cka/access-control-overview.svg" alt="access-control-overview">
</p>
<h3 id='api_tls'><a href="#api_tls">Transport Security</a></h3>
<p>
All the requests go through TLS, by default the API will run on
<code>0.0.0.0:6443</code> but this can be changed , with
<code>--secure-port</code> and the <code>--bind-address</code> flags.
</p>

<p>
When you <code>kubeadm init</code> your cluster, k8s will create its own
Certificate Authority (CA) and its key (<code>/etc/kubernetes/pki/ca.crt and
/etc/kubernetes/pki/ca.key</code> respectively). It will use this to sign the
certificates used by the API server.
</p>

<p>
Inside your <code>.kube/config</code> file you will need a copy of that
certificate, this verifies that the API's certificate is authentic and was
signed with the clusters CA.
</p>

<h3 id='api_authentication'><a href="#api_authentication">Authentication</a></h3>
<p>
Once we have TLS, we can continue with authentication. The cluster admin may
setup different authentication modules, if so they will be tried sequentially
to see any suffices.
</p>
<p>
K8s may use the whole http request to authenticate, although most modules only
use the headers.
</p>
<p>
If all the modules failed, then a <code>401</code> will be returned. If it is
successful, the user is authenticated as an specific <code>username</code>.
</p>

<h3 id='api_authorization'><a href="#api_authorization">Authorization</a></h3>
<p>
Once the request has passed the authentication stage, it is time to see if it
can in fact do the action it was trying to accomplish. The request will must
include its username, a requested action, and the resource affected by the
action. The request then will be authorized if there is an existing policy that
declares that the user has permissions to do the action it is intended to.
</p>

<p>
There are different authorization modules, the administrator can setup many in
one cluster, they will be tried one by one and if all fails a <code>403</code> will be
returned
</p>


<h3 id='admission_control'><a href="#admission_control">Admission Control</a></h3>

<p>
If the authorization is successful, then we jump to admission controles. They
are basically a piece of code that will check the data arriving in a request
that modifies a resource. They do not control requests to <i>read</i>
resources, only those that modify them. They usually just validate stuff. The
thing is that if one fails the request is rejected, it is not like the others
stages where we try one by one.
</p>

<h3 id='auditing'><a href="#auditing">Auditing</a></h3>
<p>
Generate a chronological set of records, documenting everything that is
happening.
</p>

<h3 id='rbac'><a href="#rbac">Using RBAC Authorization</a></h3>
<p>
Role-based access control is a way of controlling access to network resources
based on the roles an individual has. The
<code>rbac.authorization.k8s.io</code> api group, allows you to set them up
dynamically in the k8s cluster.
</p>
<h4 id='rbac_api'><a href="#rbac_api">API objects</a></h4>
<p>
RBAC introduces 4 new object types to the cluster, <code>Role</code>,
<code>ClusterRole</code>, <code>RoleBinding</code>,
<code>ClusterRoleBinding</code>.
</p>
<p>
<ul>
  <li>
    <b> <code>Role</code> and <code>ClusterRole</code></b>
    <p>
        These represent a set of permissions. The only difference between the
        two is that <code>Role</code> defines the permissions for a namespace,
        and <code>ClusterRole</code> is not limited to a namespace.
    </p>
    <ul>
      <li>
        <b><code>Role</code></b>
          <p>
            Here is the command for creating a role to get and watch all the
            pods in the nginx namespace.
            <pre>
            <code>
k create role --dry-run=client -o yaml pod-reader --resource=pod --verb=get,watch -n nginx
            </code>
            </pre>
          </p>
          <p>
            To be honest you might be better just going to the docs and copy
            the manifest from there, since it can get a bit long to write all
            the verbs and resources in one command.
          </p>
      </li>
      <li>
        <b><code>ClusterRole</code></b>
        <p>
          Since these are not bound to a namespaces you can also use them to
          set permissions to things like nodes and persistent volumes.
        </p>
        </p>
          The command is super similar, just we do not specify a namespace
          <pre>
          <code>
k create clusterrole secret-watcher --resource=secret --verb=get,list --dry-run=client -o yaml
          </code>
          </pre>
        </p>
        <p>
          Another thing specific to <code>ClusterRole</code>s is that you can
          aggregate them. When you create one you can add a label to it. Then
          you can create another one that uses that label.
          <pre>
          <code>
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring-aggregate
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.example.com/aggregate-to-monitoring: "true"
rules: []
          </code>
          </pre>
          In this example we are adding all <code>ClusterRole</code>s that have
          the label <code>rbac.example.com/aggregate-to-monitoring: "true"
          </code>.
        </p>
      </li>
    </ul>
  </li>
  <li>
    <b> <code>RoleBinding</code> and <code>ClusterRoleBinding</code></b>
    <p>
        Once you created your <code>Role</code> object you can bind it to a
        user, or service account. This makes <code>Role</code>s reusable. For
        example, you can create a <i>pod-read-only</i>, and bind it to many
        <code>subjects</code> (users, groups or service accounts).
    </p>
    <p>
        A <code>RoleBinding</code> may bind any <code>Role</code>  in the same
        namespace, buuut you can also use them to bind
        <code>ClusterRoles</code>, to a single namespace.
    </p>
    <p>
      You can create them as you would expect
      <pre>
      <code>
k create rolebinding pod-reader --dry-run=client -o yaml  --role=pod-readonly --user=jose
      </code>
      </pre>
      Remember you can always <code>--help</code> stuff, or copy an
      example from the wiki
    </p>
    <p>
      You cannot patch/edit an existing rolebinding to change the roles. You
      have to delete it and create one again. There is this <code>kubectl auth
      reconcile</code> which will do that for you.
    </p>
  </li>
</ul>
</p>
<p>
One last tip, you can always do
<pre>
<code>
 k auth can-i get pod/logs --as="some-subject" -n "ns" # can-i verb resource
</code>
</pre>
To check if the role is working as expected.
<p>

<h3 id='sa'><a href="#sa">Service Account</a></h3>

<p>
A service account is a non-human account that provides an identity in a k8s
cluster. Pods can use them to do requests against k8s api, to authenticate
against a image registry.
</p>

<p>
They are represented in the k8s cluster with the <code>ServiceAccount</code>
object. They are <b>namespaced</b>, <b>lightweight</b>, <b>portable</b>.
</p>

<p>
There is also this <code>default</code> service account created in every
namespace. If you try to delete it the control plane replaces it. This account
is assign to all pods if you do not manually assign one, and has api discovery
permissions.
</p>

<p>
You can use <code>RBAC</code> to add roles to it. It is just another subject
you can include in the roles manifests.
</p>

<p>
To use one you just have to:
<ul>
    <li>
        <p>Create the service account, in a declarative or imperative way</p>
    </li>
    <li>
        <p>Give it roles with RBAC</p>
    </li>
    <li>
        <p>Assign it to a pod during its creation</p>
    </li>
</ul>
If you need its identity for an external service you need to get a token.
<pre>
<code>
k create token "sa-name" -n test-token
</code>
</pre>
</p>

<p>
To assign one to a pod just add the <code>spec.serviceAccountName</code> field.
</p>
</section>

<h2 id='ops_crd'><a href='#ops_crd'>Operators and Custom Resources</a></h2>
<section>
<h3 id='cr'><a href='#cr'>Custom Resources</a></h3>
<p>
A <i>resource</i> is an endpoint in k8s api, that manages objects of the same type.
One example is the <code>pods</code> resource; it is an endpoint of the api and
you use it to create, destroy, list pod objects.
</p>

<p>
Then a <i>custom resource</i> is an extension of k8s native api. You can
create your own resources for your own needs. Custom resources can be created
and destroyed dynamically on a running cluster, and once installed you can use
<code>kubectl</code> to manage them as you would manage any other resource.
</p>
<p>
Say, you might have one to create a <i>database</i> custom resource to
represent and manage it inside of your cluster.
</p>
<h3 id='cc'><a href='#cc'>Custom Controllers</a></h3>
<p>
A <i>custom resource</i> by itself will only represent some structured data. To
make them work in a true declarative state you need to add also a controller.
</p>
<p>
In an imperative API you tell the server to do something and it does it. In a
declarative api, like k8s is, you tell it the state you want to accomplish, in
this case using the <i>custom resources</i> endpoints, and then there will be a
controller that makes sure that state is true.
</p>
<hr>
<small>
One small disclaimer, using a custom resource is not a one size fits all
solution, you have to check if you really need to implement this as a CRD or
maybe just a separated API or even a configmap would do the trick, the <a
href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#should-i-add-a-custom-resource-to-my-kubernetes-cluster">k8s
docs</a> have a section to help you decide whether you really need them or not.

</small>
<hr>
<p>
It is outside of the scope of the exam but there are two ways of creating a
custom resource.
<ul>
    <li>
        <p>Using the <code>CustomResourceDefinition</code> api resource</p>
    </li>
    <li>
        <p>Using API server aggregation</p>
    </li>
</ul>
The first one is simple as you do not need to code anything just define the
custom resource in a manifest and the k8s api will handle storage and stuff
like that. Go to the official docs for more info.
<p>
<h3 id='ops'><a href='#ops'>Operator Pattern</a></h3>
<p>
The operator pattern is creating a custom controller to manage a custom
resource.
</p>
<p>
One example would be, <i>deploying an application on demand</i>. This would
look something like, we have a new custom resource called
<code>ApplicationDeployment</code> where the developer specifies the application
they want to deploy. Now when they <code>k apply -f</code> it, there would be a
controller that takes care of all the deployment of the app.
</p>
<p>
There are many operators already created by the community, you can find several
in the <a href="https://operatorhub.io">OperatorHub</a>. A popular one is
ArgoCD, this defines custom resources such as <code>Application</code> where
you can point to a git repository , and it will make sure the code is in sync
with that repository among other things. Popular on organizations using GitOps.
</p>
</section>

<h2 id='helm_kustomize'><a href='#helm_kustomize'>Helm and Kustomize</a></h2>
<section>
<h3 id='helm'><a href='#helm'>Helm</a></h3>
<p>
Helm is a package manager for k8s.  This means that you can use it similar
to <code>apt</code> or <code>dnf</code> to install full working packages in the
k8s cluster.
</p>
<p>
Usually a deployment of  a full service in a k8s cluster would involve multiple
resources, <code>services</code> <code>pods</code> <code>configmaps</code>. It
would be a bit complicated to deploy all of them using <code>kubectl</code>.
With helm you can deploy full working solutions with just a few commands.
</p>
<p>
Say you want to deploy jenkins in your cluster. You could just look in the <a
href="https://artifacthub.io">ArtifactHub</a> for jenkins,  and follow the
instructions for installing the chart. It typically looks something like the
following.
<ul>
    <li>
        <p>
          We first need to add the repo for helm to keep track of it.
          <pre><code>
helm repo add jenkins https://charts.jenkins.io
helm repo update
          </code></pre>
        </p>
    </li>
    <li>
        <p>
        Then you can just install it specifying a name for the release. Do not
        forget that you are using your <code>kubeconfig</code> configuration so
        the namespace and cluster you are pointing to will be the target of
        this operation.
        <pre><code>
helm install my-jenkins jenkins/jenkins --version 5.8.25 # helm install [RELEASE_NAME] jenkins/jenkins [flags]
        </code></pre>
        </p>
    </li>
    <li>
        <p>
        It will create all the k8s resources needed for it to work. The
        cool thing about this is that you can customize it a bit by passing
        values to certain variables for the package. Say you want to change
        the admin user, it varies on the package of course but here you can
        do something like:
        <pre><code>
helm install my-jenkins jenkinsci/jenkins --version 4.6.4 \
    --set controller.adminUser=boss --set controller.adminPassword=password \
    -n jenkins --create-namespace
        </code></pre>
        </p>
    </li>
    <li>
        <p>
        You can discover a list of all the values too.
            <pre><code>
helm show values jenkinsci/jenkins
            </code></pre>
        </p>
    </li>
    <li>
        You can list the installed packages
        <p>
            <pre><code>
helm list
            </code></pre>
        </p>
    </li>
    <li>
        There is also a simple way to upgrade a release
        <p>
            <pre><code>
helm repo update; # so we have the most up-to-date version
helm upgrade my-jenkins jenkinsci/jenkins --version 5.8.26
            </code></pre>
        </p>
    </li>
    <li>
        Finally you can remove them by just doing the uninstall subcommand
        <p>
            <pre><code>
helm uninstall my-jenkins
            </code></pre>
        </p>
    </li>
</ul>
</p>
<h3 id='kustomize'><a href='#kustomize'>Kustomize</a></h3>
<p>
Kustomize allow you to manage multiple k8s manifests in an easy way. It has
different capabilities.
<ul>
    <li>
        <p>
            You can build <code>configmaps</code> and other resources out of
            files.
        </p>
    </li>
    <li>
        <p>
            You can patch different values, say the DNS for an application
            based on different overlays/environments.
        </p>
    </li>
</ul>
It is really not worth going that much into detail since this will likely not
come into the certification.
</p>
<p>
Just a few quick things, the heart of this is the
<code>kustomization.yaml</code> file, there you will list all the resources
kustomize will use to render the templates.
</p>
<p>
You can also render how the manifests would look without having to apply them
with
<pre><code>
kustomize bulid /path/to/kustomization.yaml # or
k kustomize /path/to/kustomization.yaml
</code></pre>
</p>
<p>
Here is a short example on how you can start using this, say to add the same
namespace to two different manfiests.
<pre><code>
% tail -n +1  kustomization.yaml pod.yaml configmap.yaml
== kustomization.yaml ==
namespace: kustom
resources:
- pod.yaml
- configmap.yaml

== pod.yaml ==
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx:1.21.1
    name: nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

== configmap.yaml ==
apiVersion: v1
data:
  dir: /etc/logs/traffic-log.txt #/etc/logs/traffic.log
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: logs-config
</code></pre>
</p>
</section>
<hr>
<h2 id='workloads'><a href='#workloadse'>Workloads</a></h2>
<section>
<p>
A workload is an application running inside a k8s cluster. Wether your
application has different components running or just one, you will run it
inside a set of Pods. A Pod is nothing more than a set of containers.
</p>

<p>
Pods have a defined life-cycle, meaning if you kill one or it dies do to some
issue, it is not going to respawn by itself or anything.
</p>

<p>
To make life easier, k8s has a set of different controllers that will help
manage this pods. Say, always keeping 3 of them alive, even if one is killed,
spin up another one to take its place. You can use <i>workload resources</i> to
make this happen. The workload resources will configure these controllers
depending on what you want to do. We will go more in depth on each, but here is
a brief intro into each of them.
<p>
<ul>
    <li>
        <p>
            <b>Deployment</b> and <b>ReplicaSet</b>, these are good more
            managing workloads where pods are replaceable/interchangeable,
            <i>stateless</i> applications.
        </p>
    </li>
    <li>
        <p>
           <b>StatefulSet</b>, this will help you run applications where pods
           do keep track of the state. Useful when mounting Persistent Volumes
           to different pods, so they stay consistent.
        </p>
    </li>
    <li>
        <p>
          <b>DaemonSet</b>, pods that provide some functionality to Nodes,
          maybe for networking, or to manage the node. These are like daemons
          that will be assigned to each Node.
        </p>
    </li>
    <li>
        <p>
          <b>Job</b> and <b>CronJob</b>, define tasks that will run until
          completion and then stop.
        </p>
    </li>
</ul>

<h3 id='pods'><a href='#pods'>Pods</a></h3>
<p>
A pod is like a set of containers with shared namespaces and shared file
systems. You can run just one or multiple containers in one Pod.
</p>
<h3 id='pods_life'><a href='#pods_life'>A Pod's Lifecycle</a></h3>
<p>
Pods are consider ephemeral, pods are created, assigned a unique id (UID),
scheduled to run to nodes where they will live until their termination. If a
node dies, the pods that lived there, or were scheduled to live there, will be
marked for deletion.
</p>
<p>
While a pod is running, <code>kubelet</code> can restart its containers to
handle some kind of faults.
</p>
<p>
Pods are only scheduled one in their lifetime; assigning a pod to a node is
called <i>binding</i>, and the process of selecting which node the pod should
go to is known as <i>scheduling</i>. Once a pod is scheduled to a node they are
bound until either of them dies.
</p>
<p>
A pod is never "re-scheduled", it is simply killed and replaced by maybe a
super similar one but the UID will be different.
</p>
<p>
<h4 id='pods_phases'><a href='#pods_phases'>Pod phases</a></h4>
<p>
There are several pods phases:
</p>
<table>
    <thead>
    <tr>
        <th>Phase</th>
        <th>Description</th>
    </tr>
    </thead>
    <tbody>
    <tr>
        <td>Pending</td>
        <td>
            The Pod has been accepted by k8s, but one or more containers are
            not ready to run. This means it might be waiting for scheduling or
            downloading an image from a registry.
        </td>
    </tr>
    <tr>
        <td>Running</td>
        <td>
            The Pod has been bound to a Node, all the containers have been
            created. At least one of them is running, or in the process of
            starting/restarting.
        </td>
    </tr>
    <tr>
        <td>Succeeded</td>
        <td>
            All containers in the Pod have been terminated in success.
        </td>
    </tr>
    <tr>
        <td>Failed</td>
        <td>
            All containers in the Pod have been terminated, but one ore more
            terminated in failure.
        </td>
    </tr>
    <tr>
        <td>Unknown</td>
        <td>
            We could not get the state of the pod, usually an error with
            communicating with the Node the pod is running on.
        </td>
    </tr>
    </tbody>
</table>
<blockquote>
<code>CrashLoopBackOff</code> and <code>Terminating</code> are not actually
<b>phases</b> of a pod. Make sure to not confuse status with phase.
</blockquote>
<h4 id='pods_handle_issues'><a href='#pods_handle_issues'>Pods handling issues</a></h4>
<p>
Similar to every living thing on this green Earth, a Pod will be presented with
issues along its time in this world filled with thorns and thistles. Maybe, as
us, even its own life will depend on how well it is able to solve them. This
unnecessary biblical de-tour begs the question, how does it handle problems
with containers?
</p>
<p>
The pods <code>spec</code> has a <code>restartPolicy</code>. This will
determine how k8s reacts to containers exiting due to errors.
<ol>
    <li>
        <p>
            <b>Initial Crash</b>, k8s immediately tries to restart it based on
            the <code>restartPolicy</code>
        </p>
    </li>
    <li>
        <p>
            <b>Repeated Crashes</b>, if it keeps failing, it will add an
            exponential backoff delay for the next restarts
        </p>
    </li>
    <li>
        <p>
            <b>CrashLoopBackOff state</b>, this indicates the backoff delay
            mechanism is in effect.

        </p>
    </li>
    <li>
        <p>
            <b>Backoff reset</b>, if a container manages to stay alive for a
            certain duration of time, the backoff delay is restarted.
        </p>
    </li>
</ol>
</p>
<p>
Troubleshooting is its own separated section, but here are some reasons a Pod
might be <code>CrashLoopBackOff </code>ing.
<ul>
    <li>
        <p>Application errors are causing the container to exit</p>
    </li>
    <li>
        <p>Configuration errors, missing files, or env vars</p>
    </li>
    <li>
        <p>
            Resources, the container may not have enough memory or cpu to start
        </p>
    </li>
    <li>
        <p>
            Healthchecks are failing if the application doesn't start serving
            in time.
        </p>
    </li>
</ul>
</p>
<p>
How to debug this? Check the <code>logs</code>, <code>events</code>, ensure the
configuration is set up properly, check resources limits, debug application.
Maybe even run the image locally, see if it is working fine.
</p>
<p>
A <code>restartPolicy </code> can be <code>Never</code>, <code>Always</code>,
<code>OnFailure</code>.
</p>

<h4 id='container_probes'><a href='#container_probes'>Container Probes</a></h4>
<p>
A probe is a diagnostic periodically performed by the kubelet. There are three
types the <code>livenessProbe</code>, <code>readinessProbe</code>,
<code>startupProbe</code>.
</p>

<p>
Pretty self explanatory, maybe the only thing to clarify is that the
<code>startupProbe</code> indicates if the app inside a container started. All
the other probes will be disabled until this is done. Usually this one is used
for containers that take a long time to start.
</p>

<p>
And the <code>readinessProbe</code> indicates whether the container is ready to
respond to requests.
</p>

<p>
There are 4 check mechanisms.
<ol>
    <li>
        <p>
            <code>exec</code>: exec a command inside the container, if
            successful return <code>0</code>.
        </p>
    </li>
    <li>
        <p><code>grpc</code>: performs remote call using gRPC.</p>
    </li>
    <li>
        <p>
            <code>httpGet</code>: makes a http GET request against the pod ip
            to a given endpoint.
        </p>
    </li>
    <li>
        <p>
            <code>tcpSocket</code>: perform  a tcp check, considers successful
            if the port is open
        </p>
    </li>
</ol>
</p>
</section>

<hr>
<h3 id='table'>~ Table of Contents</h3>
<ul>
  <li><a href='#exam_details'>Exam Details</a></li>
  <li>
    <a href='#k8s_nutshell'>K8s in a Nutshell</a>
    <ul>
      <li><a href='#features'>Features</a></li>
      <li><a href='#high_level_arch'>High-Level Architecture</a></li>
      <li><a href='#control_plane_nodes'>Control Plane Nodes</a></li>
      <li><a href='#shared_components'>Components Shared by Nodes</a></li>
      <li><a href='#advantages'>Advantages of Using k8s</a></li>
    </ul>
  </li>
  <li>
    <a href='#interacting_k8s'>Interacting with K8s</a>
    <ul>
      <li><a href='#api_primitives'>API Primitives and Objects</a></li>
      <li><a href='#kubectl'>kubectl</a></li>
      <li>
        <a href='#managing_objects'>Managing Objects</a>
        <ul>
          <li><a href='#imperative'>Imperative</a></li>
          <li><a href='#declarative'>Declarative</a></li>
          <li><a href='#hybrid'>Hybrid</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <a href='#cluster_installation'>Cluster Installation and Upgrade</a>
    <ul>
      <li><a href='#provision_infra'>Get Infrastructure Ready</a></li>
      <li><a href='#extension_interfaces'>Extension Interfaces</a></li>
      <li><a href='#setup_cluster'>Setup Cluster</a></li>
      <li><a href='#highly_available'>Highly Available (HA) Cluster</a>
        <ul>
          <li><a href="stacked_etcd">Stacked etcd topology</a></li>
          <li><a href="external_etcd">External etcd topology</a></li>
        </ul>
      </li>
      <li><a href='#upgrading_cluster'>Upgrading Cluster Version</a>
        <ul>
          <li><a href="#upgrade_control_planes">Upgrade Control Planes</a></li>
          <li><a href="#upgrade_workers">Upgrade Workers</a></li>
        </ul>
      </li>
      </li>
      <li>
        <a href='#etcd'>etcd</a>
        <ul>
          <li><a href="#backup_etcd">Backing up and etcd cluster</a></li>
          <li><a href="#restore_etcd">Restore from snapshot</a></li>
        </ul>
      </li>
      <li>
        <a href='#auth'>Control Access to the k8s API</a>
        <ul>
          <li><a href="#api_tls">Transport Security</a></li>
          <li><a href="#api_authentication">Authentication</a></li>
          <li><a href="#api_authorization">Authorization</a></li>
          <li><a href="#admission_control">Admission Control</a></li>
          <li><a href="#auditing">Auditing</a></li>
          <li><a href="#rbac">Using RBAC Authorization</a></li>
          <li><a href="#sa">Service Accounts</a></li>
        </ul>
      </li>
      <li>
        <a href='#ops_crd'>Operators and Custom Resources</a>
        <ul>
          <li><a href="#cr">Custom Resources</a></li>
          <li><a href="#cc">Custom Controllers</a></li>
          <li><a href="#ops">Operator pattern</a></li>
        </ul>
      </li>
      <li>
        <a href='#helm_kustomize'>Helm and Kustomize</a>
        <ul>
          <li><a href="#helm">Helm</a></li>
          <li><a href="#kustomize">Kustomize</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <a href='#workloads'>Workloads</a>
    <ul>
      <li>
        <a href='#pods'>Pods</a>
        <ul>
          <li><a href="#pods_life">A Pod's Lifecycle</a></li>
          <li><a href="#pods_phases">A Pod's Lifecycle</a></li>
          <li><a href="#pods_handle_issues">Pods handling issues</a></li>
          <li><a href='#container_probes'>Container Probes</a></li>
        </ul>
      </li>
      <li><a href='#workload_management'>Workload Management</a></li>
    </ul>
  </li>
</ul>
<p><a href='#top'> go to the top</a></p>
</body>
</html>
